{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7380c873",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.012046,
     "end_time": "2025-12-12T01:13:03.503084",
     "exception": false,
     "start_time": "2025-12-12T01:13:03.491038",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**References**\n",
    "- [MABe Nearest Neighbors: The Original ⭐️⭐️⭐️⭐️⭐️](https://www.kaggle.com/code/ambrosm/mabe-nearest-neighbors-the-original)\n",
    "- [MABe EDA which makes sense ⭐️⭐️⭐️⭐️⭐️](https://www.kaggle.com/code/ambrosm/mabe-eda-which-makes-sense)\n",
    "- [MABe Validated baseline without machine learning](https://www.kaggle.com/code/ambrosm/mabe-validated-baseline-without-machine-learning)\n",
    "- [Squeeze GBT](https://www.kaggle.com/code/cody11null/squeeze-gbt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174c2fbc",
   "metadata": {
    "papermill": {
     "duration": 0.010142,
     "end_time": "2025-12-12T01:13:03.523401",
     "exception": false,
     "start_time": "2025-12-12T01:13:03.513259",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Imports and configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58be0938",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T01:13:03.544805Z",
     "iopub.status.busy": "2025-12-12T01:13:03.543880Z",
     "iopub.status.idle": "2025-12-12T01:13:13.700476Z",
     "shell.execute_reply": "2025-12-12T01:13:13.699519Z"
    },
    "papermill": {
     "duration": 10.169313,
     "end_time": "2025-12-12T01:13:13.702329",
     "exception": false,
     "start_time": "2025-12-12T01:13:03.533016",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /kaggle/input/koolbox-offline\r\n",
      "Processing /kaggle/input/koolbox-offline/koolbox-0.1.3-py3-none-any.whl\r\n",
      "Processing /kaggle/input/koolbox-offline/scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from koolbox)\r\n",
      "Requirement already satisfied: optuna>=4.2.1 in /usr/local/lib/python3.11/dist-packages (from koolbox) (4.5.0)\r\n",
      "Requirement already satisfied: pandas>=2.2.3 in /usr/local/lib/python3.11/dist-packages (from koolbox) (2.2.3)\r\n",
      "Requirement already satisfied: numpy>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from koolbox) (1.26.4)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.4->koolbox) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.4->koolbox) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.4->koolbox) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.4->koolbox) (2025.3.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.4->koolbox) (2022.3.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.4->koolbox) (2.4.1)\r\n",
      "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna>=4.2.1->koolbox) (1.17.1)\r\n",
      "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna>=4.2.1->koolbox) (6.10.1)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna>=4.2.1->koolbox) (25.0)\r\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna>=4.2.1->koolbox) (2.0.41)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna>=4.2.1->koolbox) (4.67.1)\r\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna>=4.2.1->koolbox) (6.0.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.3->koolbox) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.3->koolbox) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.3->koolbox) (2025.2)\r\n",
      "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.5.2->koolbox) (1.15.3)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.5.2->koolbox) (1.5.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.5.2->koolbox) (3.6.0)\r\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna>=4.2.1->koolbox) (1.3.10)\r\n",
      "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna>=4.2.1->koolbox) (4.15.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.2.3->koolbox) (1.17.0)\r\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna>=4.2.1->koolbox) (3.2.3)\r\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.4->koolbox) (2025.3.0)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.4->koolbox) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.4->koolbox) (2022.3.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.26.4->koolbox) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.26.4->koolbox) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.26.4->koolbox) (2024.2.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna>=4.2.1->koolbox) (3.0.3)\r\n",
      "Installing collected packages: scikit-learn, koolbox\r\n",
      "  Attempting uninstall: scikit-learn\r\n",
      "    Found existing installation: scikit-learn 1.2.2\r\n",
      "    Uninstalling scikit-learn-1.2.2:\r\n",
      "      Successfully uninstalled scikit-learn-1.2.2\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "category-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.7.2 which is incompatible.\r\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\r\n",
      "sklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.2 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed koolbox-0.1.3 scikit-learn-1.7.2\r\n"
     ]
    }
   ],
   "source": [
    "!pip install koolbox --no-index --find-links=/kaggle/input/koolbox-offline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b758ee9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T01:13:13.725746Z",
     "iopub.status.busy": "2025-12-12T01:13:13.725404Z",
     "iopub.status.idle": "2025-12-12T01:13:23.410144Z",
     "shell.execute_reply": "2025-12-12T01:13:23.409214Z"
    },
    "papermill": {
     "duration": 9.698777,
     "end_time": "2025-12-12T01:13:23.411846",
     "exception": false,
     "start_time": "2025-12-12T01:13:13.713069",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.base import clone\n",
    "from xgboost import XGBClassifier\n",
    "from tqdm.notebook import tqdm\n",
    "from koolbox import Trainer\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import itertools\n",
    "import warnings\n",
    "import optuna\n",
    "import joblib\n",
    "import glob\n",
    "import gc\n",
    "from scipy.ndimage import label as scipy_label\n",
    "\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c9d5b04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T01:13:23.435846Z",
     "iopub.status.busy": "2025-12-12T01:13:23.434678Z",
     "iopub.status.idle": "2025-12-12T01:13:23.440422Z",
     "shell.execute_reply": "2025-12-12T01:13:23.439571Z"
    },
    "papermill": {
     "duration": 0.018804,
     "end_time": "2025-12-12T01:13:23.441755",
     "exception": false,
     "start_time": "2025-12-12T01:13:23.422951",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2dbabfe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T01:13:23.464357Z",
     "iopub.status.busy": "2025-12-12T01:13:23.464006Z",
     "iopub.status.idle": "2025-12-12T01:13:28.946016Z",
     "shell.execute_reply": "2025-12-12T01:13:28.944917Z"
    },
    "papermill": {
     "duration": 5.495425,
     "end_time": "2025-12-12T01:13:28.947668",
     "exception": false,
     "start_time": "2025-12-12T01:13:23.452243",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available, using CPU.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Kiểm tra GPU khả dụng\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "\n",
    "if USE_GPU:\n",
    "    print(f\"GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "    # Cấu hình cho XGBoost sử dụng GPU\n",
    "    XGB_TREE_METHOD = 'gpu_hist' \n",
    "    XGB_DEVICE = 'cuda'\n",
    "else:\n",
    "    print(\"GPU not available, using CPU.\")\n",
    "    XGB_TREE_METHOD = 'hist'\n",
    "    XGB_DEVICE = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57ee47e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T01:13:28.970311Z",
     "iopub.status.busy": "2025-12-12T01:13:28.969701Z",
     "iopub.status.idle": "2025-12-12T01:13:28.976123Z",
     "shell.execute_reply": "2025-12-12T01:13:28.975310Z"
    },
    "papermill": {
     "duration": 0.019362,
     "end_time": "2025-12-12T01:13:28.977553",
     "exception": false,
     "start_time": "2025-12-12T01:13:28.958191",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    train_path = \"/kaggle/input/MABe-mouse-behavior-detection/train.csv\"\n",
    "    test_path = \"/kaggle/input/MABe-mouse-behavior-detection/test.csv\"\n",
    "    train_annotation_path = \"/kaggle/input/MABe-mouse-behavior-detection/train_annotation\"\n",
    "    train_tracking_path = \"/kaggle/input/MABe-mouse-behavior-detection/train_tracking\"\n",
    "    test_tracking_path = \"/kaggle/input/MABe-mouse-behavior-detection/test_tracking\"\n",
    "\n",
    "    model_path = \"/kaggle/input/lgbm-v1-dataset\"\n",
    "    model_name = \"lgbm\"\n",
    "    \n",
    "    # mode = \"validate\"\n",
    "    mode = \"submit\"\n",
    "\n",
    "    n_splits=2\n",
    "\n",
    "    cv = StratifiedGroupKFold(n_splits=2, shuffle=True, random_state=42)\n",
    "\n",
    "    model = lgb.LGBMClassifier(\n",
    "        verbosity=-1,         \n",
    "        random_state=43,\n",
    "        n_estimators=350, \n",
    "        learning_rate=0.1, \n",
    "        max_depth=8,\n",
    "        min_child_weight=5, \n",
    "        subsample=0.8, \n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.07,\n",
    "        reg_lambda=0.05,\n",
    "        n_jobs=-1,\n",
    "        device_type='cpu' \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed87b18",
   "metadata": {
    "papermill": {
     "duration": 0.01064,
     "end_time": "2025-12-12T01:13:28.998878",
     "exception": false,
     "start_time": "2025-12-12T01:13:28.988238",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Competition Metric (F-Beta Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b30e7310",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T01:13:29.021893Z",
     "iopub.status.busy": "2025-12-12T01:13:29.021580Z",
     "iopub.status.idle": "2025-12-12T01:13:29.709721Z",
     "shell.execute_reply": "2025-12-12T01:13:29.708725Z"
    },
    "papermill": {
     "duration": 0.702172,
     "end_time": "2025-12-12T01:13:29.711509",
     "exception": false,
     "start_time": "2025-12-12T01:13:29.009337",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"F Beta customized for the data format of the MABe challenge.\"\"\"\n",
    "\n",
    "import json\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "\n",
    "class HostVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def single_lab_f1(lab_solution: pl.DataFrame, lab_submission: pl.DataFrame, beta: float = 1) -> float:\n",
    "    label_frames: defaultdict[str, set[int]] = defaultdict(set)\n",
    "    prediction_frames: defaultdict[str, set[int]] = defaultdict(set)\n",
    "\n",
    "    for row in lab_solution.to_dicts():\n",
    "        label_frames[row['label_key']].update(range(row['start_frame'], row['stop_frame']))\n",
    "\n",
    "    for video in lab_solution['video_id'].unique():\n",
    "        active_labels: str = lab_solution.filter(pl.col('video_id') == video)['behaviors_labeled'].first()  # ty: ignore\n",
    "        active_labels: set[str] = set(json.loads(active_labels))\n",
    "        predicted_mouse_pairs: defaultdict[str, set[int]] = defaultdict(set)\n",
    "\n",
    "        for row in lab_submission.filter(pl.col('video_id') == video).to_dicts():\n",
    "            # Since the labels are sparse, we can't evaluate prediction keys not in the active labels.\n",
    "            if ','.join([str(row['agent_id']), str(row['target_id']), row['action']]) not in active_labels:\n",
    "                continue\n",
    "\n",
    "            new_frames = set(range(row['start_frame'], row['stop_frame']))\n",
    "            # Ignore truly redundant predictions.\n",
    "            new_frames = new_frames.difference(prediction_frames[row['prediction_key']])\n",
    "            prediction_pair = ','.join([str(row['agent_id']), str(row['target_id'])])\n",
    "            if predicted_mouse_pairs[prediction_pair].intersection(new_frames):\n",
    "                # A single agent can have multiple targets per frame (ex: evading all other mice) but only one action per target per frame.\n",
    "                raise HostVisibleError('Multiple predictions for the same frame from one agent/target pair')\n",
    "            prediction_frames[row['prediction_key']].update(new_frames)\n",
    "            predicted_mouse_pairs[prediction_pair].update(new_frames)\n",
    "\n",
    "    tps = defaultdict(int)\n",
    "    fns = defaultdict(int)\n",
    "    fps = defaultdict(int)\n",
    "    for key, pred_frames in prediction_frames.items():\n",
    "        action = key.split('_')[-1]\n",
    "        matched_label_frames = label_frames[key]\n",
    "        tps[action] += len(pred_frames.intersection(matched_label_frames))\n",
    "        fns[action] += len(matched_label_frames.difference(pred_frames))\n",
    "        fps[action] += len(pred_frames.difference(matched_label_frames))\n",
    "\n",
    "    distinct_actions = set()\n",
    "    for key, frames in label_frames.items():\n",
    "        action = key.split('_')[-1]\n",
    "        distinct_actions.add(action)\n",
    "        if key not in prediction_frames:\n",
    "            fns[action] += len(frames)\n",
    "\n",
    "    action_f1s = []\n",
    "    for action in distinct_actions:\n",
    "        if tps[action] + fns[action] + fps[action] == 0:\n",
    "            action_f1s.append(0)\n",
    "        else:\n",
    "            action_f1s.append((1 + beta**2) * tps[action] / ((1 + beta**2) * tps[action] + beta**2 * fns[action] + fps[action]))\n",
    "    return sum(action_f1s) / len(action_f1s)\n",
    "\n",
    "\n",
    "def mouse_fbeta(solution: pd.DataFrame, submission: pd.DataFrame, beta: float = 1) -> float:\n",
    "   \n",
    "    if len(solution) == 0 or len(submission) == 0:\n",
    "        raise ValueError('Missing solution or submission data')\n",
    "\n",
    "    expected_cols = ['video_id', 'agent_id', 'target_id', 'action', 'start_frame', 'stop_frame']\n",
    "\n",
    "    for col in expected_cols:\n",
    "        if col not in solution.columns:\n",
    "            raise ValueError(f'Solution is missing column {col}')\n",
    "        if col not in submission.columns:\n",
    "            raise ValueError(f'Submission is missing column {col}')\n",
    "\n",
    "    solution: pl.DataFrame = pl.DataFrame(solution)\n",
    "    submission: pl.DataFrame = pl.DataFrame(submission)\n",
    "    assert (solution['start_frame'] <= solution['stop_frame']).all()\n",
    "    assert (submission['start_frame'] <= submission['stop_frame']).all()\n",
    "    solution_videos = set(solution['video_id'].unique())\n",
    "    # Need to align based on video IDs as we can't rely on the row IDs for handling public/private splits.\n",
    "    submission = submission.filter(pl.col('video_id').is_in(solution_videos))\n",
    "\n",
    "    solution = solution.with_columns(\n",
    "        pl.concat_str(\n",
    "            [\n",
    "                pl.col('video_id').cast(pl.Utf8),\n",
    "                pl.col('agent_id').cast(pl.Utf8),\n",
    "                pl.col('target_id').cast(pl.Utf8),\n",
    "                pl.col('action'),\n",
    "            ],\n",
    "            separator='_',\n",
    "        ).alias('label_key'),\n",
    "    )\n",
    "    submission = submission.with_columns(\n",
    "        pl.concat_str(\n",
    "            [\n",
    "                pl.col('video_id').cast(pl.Utf8),\n",
    "                pl.col('agent_id').cast(pl.Utf8),\n",
    "                pl.col('target_id').cast(pl.Utf8),\n",
    "                pl.col('action'),\n",
    "            ],\n",
    "            separator='_',\n",
    "        ).alias('prediction_key'),\n",
    "    )\n",
    "\n",
    "    lab_scores = []\n",
    "    for lab in solution['lab_id'].unique():\n",
    "        lab_solution = solution.filter(pl.col('lab_id') == lab).clone()\n",
    "        lab_videos = set(lab_solution['video_id'].unique())\n",
    "        lab_submission = submission.filter(pl.col('video_id').is_in(lab_videos)).clone()\n",
    "        lab_scores.append(single_lab_f1(lab_solution, lab_submission, beta=beta))\n",
    "\n",
    "    return sum(lab_scores) / len(lab_scores)\n",
    "\n",
    "\n",
    "def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str, beta: float = 1) -> float:\n",
    "    \"\"\"\n",
    "    F1 score for the MABe Challenge\n",
    "    \"\"\"\n",
    "    solution = solution.drop(row_id_column_name, axis='columns', errors='ignore')\n",
    "    submission = submission.drop(row_id_column_name, axis='columns', errors='ignore')\n",
    "    return mouse_fbeta(solution, submission, beta=beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f20a1f",
   "metadata": {
    "papermill": {
     "duration": 0.010103,
     "end_time": "2025-12-12T01:13:29.732288",
     "exception": false,
     "start_time": "2025-12-12T01:13:29.722185",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "213523c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T01:13:29.753998Z",
     "iopub.status.busy": "2025-12-12T01:13:29.753575Z",
     "iopub.status.idle": "2025-12-12T01:13:29.903735Z",
     "shell.execute_reply": "2025-12-12T01:13:29.902958Z"
    },
    "papermill": {
     "duration": 0.163154,
     "end_time": "2025-12-12T01:13:29.905488",
     "exception": false,
     "start_time": "2025-12-12T01:13:29.742334",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(CFG.train_path)\n",
    "train['n_mice'] = 4 - train[['mouse1_strain', 'mouse2_strain', 'mouse3_strain', 'mouse4_strain']].isna().sum(axis=1)\n",
    "train_without_mabe22 = train.query(\"~lab_id.str.startswith('MABe22_')\")\n",
    "\n",
    "test = pd.read_csv(CFG.test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b708d6ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T01:13:29.928755Z",
     "iopub.status.busy": "2025-12-12T01:13:29.928005Z",
     "iopub.status.idle": "2025-12-12T01:13:29.936815Z",
     "shell.execute_reply": "2025-12-12T01:13:29.935952Z"
    },
    "papermill": {
     "duration": 0.022197,
     "end_time": "2025-12-12T01:13:29.938347",
     "exception": false,
     "start_time": "2025-12-12T01:13:29.916150",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "body_parts_tracked_list = list(np.unique(train.body_parts_tracked))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c470efc1",
   "metadata": {
    "papermill": {
     "duration": 0.012461,
     "end_time": "2025-12-12T01:13:29.964624",
     "exception": false,
     "start_time": "2025-12-12T01:13:29.952163",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Creating solution data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf3c0ff5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T01:13:29.986967Z",
     "iopub.status.busy": "2025-12-12T01:13:29.986634Z",
     "iopub.status.idle": "2025-12-12T01:13:29.993722Z",
     "shell.execute_reply": "2025-12-12T01:13:29.992776Z"
    },
    "papermill": {
     "duration": 0.020333,
     "end_time": "2025-12-12T01:13:29.995336",
     "exception": false,
     "start_time": "2025-12-12T01:13:29.975003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hàm này chỉ lấy vid được annotated\n",
    "\n",
    "def create_solution_df(dataset):\n",
    "    solution = []\n",
    "    for _, row in tqdm(dataset.iterrows(), total=len(dataset)):\n",
    "    \n",
    "        lab_id = row['lab_id']\n",
    "        if lab_id.startswith('MABe22'): \n",
    "            continue\n",
    "        \n",
    "        video_id = row['video_id']\n",
    "        path = f\"{CFG.train_annotation_path}/{lab_id}/{video_id}.parquet\"\n",
    "        try:\n",
    "            annot = pd.read_parquet(path)\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "    \n",
    "        annot['lab_id'] = lab_id\n",
    "        annot['video_id'] = video_id\n",
    "        annot['behaviors_labeled'] = row['behaviors_labeled']\n",
    "        annot['target_id'] = np.where(annot.target_id != annot.agent_id, annot['target_id'].apply(lambda s: f\"mouse{s}\"), 'self')\n",
    "        annot['agent_id'] = annot['agent_id'].apply(lambda s: f\"mouse{s}\")\n",
    "        solution.append(annot)\n",
    "    \n",
    "    solution = pd.concat(solution)\n",
    "    \n",
    "    return solution\n",
    "\n",
    "if CFG.mode == 'validate':\n",
    "    solution = create_solution_df(train_without_mabe22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5e9c65",
   "metadata": {
    "papermill": {
     "duration": 0.010428,
     "end_time": "2025-12-12T01:13:30.017633",
     "exception": false,
     "start_time": "2025-12-12T01:13:30.007205",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed7f901d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T01:13:30.039980Z",
     "iopub.status.busy": "2025-12-12T01:13:30.039495Z",
     "iopub.status.idle": "2025-12-12T01:13:30.055350Z",
     "shell.execute_reply": "2025-12-12T01:13:30.054556Z"
    },
    "papermill": {
     "duration": 0.028975,
     "end_time": "2025-12-12T01:13:30.056881",
     "exception": false,
     "start_time": "2025-12-12T01:13:30.027906",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "drop_body_parts = [\n",
    "    'headpiece_bottombackleft', 'headpiece_bottombackright', \n",
    "    'headpiece_bottomfrontleft', 'headpiece_bottomfrontright', \n",
    "    'headpiece_topbackleft', 'headpiece_topbackright', \n",
    "    'headpiece_topfrontleft', 'headpiece_topfrontright', \n",
    "    'spine_1', 'spine_2', 'tail_middle_1', 'tail_middle_2', 'tail_midpoint'\n",
    "]\n",
    "\n",
    "def generate_mouse_data(dataset, traintest, traintest_directory=None, \n",
    "                       generate_single=True, generate_pair=True):\n",
    "    \n",
    "    if traintest_directory is None:\n",
    "        traintest_directory = f\"/kaggle/input/MABe-mouse-behavior-detection/{traintest}_tracking\"\n",
    "        \n",
    "    for _, row in dataset.iterrows():\n",
    "        lab_id = row.lab_id\n",
    "        if lab_id.startswith('MABe22') or type(row.behaviors_labeled) != str: \n",
    "            continue\n",
    "        \n",
    "        video_id = row.video_id\n",
    "        path = f\"{traintest_directory}/{lab_id}/{video_id}.parquet\"\n",
    "        vid = pd.read_parquet(path)\n",
    "        \n",
    "        if len(np.unique(vid.bodypart)) > 5:\n",
    "            vid = vid.query(\"~ bodypart.isin(@drop_body_parts)\")\n",
    "            \n",
    "        pvid = vid.pivot(columns=['mouse_id', 'bodypart'], index='video_frame', values=['x', 'y'])\n",
    "        \n",
    "        del vid\n",
    "        gc.collect()\n",
    "        \n",
    "        pvid = pvid.reorder_levels([1, 2, 0], axis=1).T.sort_index().T\n",
    "        pvid /= row.pix_per_cm_approx\n",
    "\n",
    "        vid_behaviors = json.loads(row.behaviors_labeled)\n",
    "        vid_behaviors = sorted(list({b.replace(\"'\", \"\") for b in vid_behaviors}))\n",
    "        vid_behaviors = [b.split(',') for b in vid_behaviors]\n",
    "        vid_behaviors = pd.DataFrame(vid_behaviors, columns=['agent', 'target', 'action'])\n",
    "        \n",
    "        if traintest == 'train':\n",
    "            try:\n",
    "                annot = pd.read_parquet(path.replace('train_tracking', 'train_annotation'))\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "\n",
    "        if generate_single:\n",
    "            vid_behaviors_subset = vid_behaviors.query(\"target == 'self'\")\n",
    "            for mouse_id_str in np.unique(vid_behaviors_subset.agent):\n",
    "                try:\n",
    "                    mouse_id = int(mouse_id_str[-1])\n",
    "                    vid_agent_actions = np.unique(vid_behaviors_subset.query(\"agent == @mouse_id_str\").action)\n",
    "                    single_mouse = pvid.loc[:, mouse_id]\n",
    "                    single_mouse_meta = pd.DataFrame({\n",
    "                        'video_id': video_id,\n",
    "                        'agent_id': mouse_id_str,\n",
    "                        'target_id': 'self',\n",
    "                        'video_frame': single_mouse.index\n",
    "                    })\n",
    "                    if traintest == 'train':\n",
    "                        single_mouse_label = pd.DataFrame(0.0, columns=vid_agent_actions, index=single_mouse.index)\n",
    "                        annot_subset = annot.query(\"(agent_id == @mouse_id) & (target_id == @mouse_id)\")\n",
    "                        for i in range(len(annot_subset)):\n",
    "                            annot_row = annot_subset.iloc[i]\n",
    "                            single_mouse_label.loc[annot_row['start_frame']:annot_row['stop_frame'], annot_row.action] = 1.0\n",
    "                        yield 'single', single_mouse, single_mouse_meta, single_mouse_label\n",
    "                    else:\n",
    "                        yield 'single', single_mouse, single_mouse_meta, vid_agent_actions\n",
    "                except KeyError:\n",
    "                    pass\n",
    "\n",
    "        if generate_pair:\n",
    "            vid_behaviors_subset = vid_behaviors.query(\"target != 'self'\")\n",
    "            if len(vid_behaviors_subset) > 0:\n",
    "                for agent, target in itertools.permutations(np.unique(pvid.columns.get_level_values('mouse_id')), 2):\n",
    "                    agent_str = f\"mouse{agent}\"\n",
    "                    target_str = f\"mouse{target}\"\n",
    "                    vid_agent_actions = np.unique(vid_behaviors_subset.query(\"(agent == @agent_str) & (target == @target_str)\").action)\n",
    "                    mouse_pair = pd.concat([pvid[agent], pvid[target]], axis=1, keys=['A', 'B'])\n",
    "                    mouse_pair_meta = pd.DataFrame({\n",
    "                        'video_id': video_id,\n",
    "                        'agent_id': agent_str,\n",
    "                        'target_id': target_str,\n",
    "                        'video_frame': mouse_pair.index,\n",
    "                        'video_frame': mouse_pair.index\n",
    "                    })\n",
    "                    if traintest == 'train':\n",
    "                        mouse_pair_label = pd.DataFrame(0.0, columns=vid_agent_actions, index=mouse_pair.index)\n",
    "                        annot_subset = annot.query(\"(agent_id == @agent) & (target_id == @target)\")\n",
    "                        for i in range(len(annot_subset)):\n",
    "                            annot_row = annot_subset.iloc[i]\n",
    "                            mouse_pair_label.loc[annot_row['start_frame']:annot_row['stop_frame'], annot_row.action] = 1.0\n",
    "                        yield 'pair', mouse_pair, mouse_pair_meta, mouse_pair_label\n",
    "                    else:\n",
    "                        yield 'pair', mouse_pair, mouse_pair_meta, vid_agent_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728d5813",
   "metadata": {
    "papermill": {
     "duration": 0.011155,
     "end_time": "2025-12-12T01:13:30.078583",
     "exception": false,
     "start_time": "2025-12-12T01:13:30.067428",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Transforming coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bf322f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T01:13:30.101951Z",
     "iopub.status.busy": "2025-12-12T01:13:30.101611Z",
     "iopub.status.idle": "2025-12-12T01:13:30.110955Z",
     "shell.execute_reply": "2025-12-12T01:13:30.110072Z"
    },
    "papermill": {
     "duration": 0.022408,
     "end_time": "2025-12-12T01:13:30.112479",
     "exception": false,
     "start_time": "2025-12-12T01:13:30.090071",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def safe_rolling(series, window, func, min_periods=None):\n",
    "    if min_periods is None:\n",
    "        min_periods = max(1, window // 4)\n",
    "    return series.rolling(window, min_periods=min_periods, center=True).apply(func, raw=True)\n",
    "\n",
    "def _scale(n_frames_at_30fps, fps, ref=30.0):\n",
    "    return max(1, int(round(n_frames_at_30fps * float(fps) / ref)))\n",
    "\n",
    "def _scale_signed(n_frames_at_30fps, fps, ref=30.0):\n",
    "    if n_frames_at_30fps == 0:\n",
    "        return 0\n",
    "    s = 1 if n_frames_at_30fps > 0 else -1\n",
    "    mag = max(1, int(round(abs(n_frames_at_30fps) * float(fps) / ref)))\n",
    "    return s * mag\n",
    "\n",
    "def _fps_from_meta(meta_df, fallback_lookup, default_fps=30.0):\n",
    "    if 'frames_per_second' in meta_df.columns and pd.notnull(meta_df['frames_per_second']).any():\n",
    "        return float(meta_df['frames_per_second'].iloc[0])\n",
    "    vid = meta_df['video_id'].iloc[0]\n",
    "    return float(fallback_lookup.get(vid, default_fps))\n",
    "\n",
    "def _speed(cx: pd.Series, cy: pd.Series, fps: float) -> pd.Series:\n",
    "    return np.hypot(cx.diff(), cy.diff()).fillna(0.0) * float(fps)\n",
    "\n",
    "def _roll_future_mean(s: pd.Series, w: int, min_p: int = 1) -> pd.Series:\n",
    "    # mean over [t, t+w-1]\n",
    "    return s.iloc[::-1].rolling(w, min_periods=min_p).mean().iloc[::-1]\n",
    "\n",
    "def _roll_future_var(s: pd.Series, w: int, min_p: int = 2) -> pd.Series:\n",
    "    # var over [t, t+w-1]\n",
    "    return s.iloc[::-1].rolling(w, min_periods=min_p).var().iloc[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d53fde0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T01:13:30.135093Z",
     "iopub.status.busy": "2025-12-12T01:13:30.134765Z",
     "iopub.status.idle": "2025-12-12T01:13:30.142798Z",
     "shell.execute_reply": "2025-12-12T01:13:30.141631Z"
    },
    "papermill": {
     "duration": 0.021722,
     "end_time": "2025-12-12T01:13:30.144802",
     "exception": false,
     "start_time": "2025-12-12T01:13:30.123080",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_curvature_features(X, center_x, center_y, fps):\n",
    "    \"\"\"Add curvature (trajectory bending) and turn rate features\"\"\"\n",
    "    vel_x = center_x.diff()\n",
    "    vel_y = center_y.diff()\n",
    "    acc_x = vel_x.diff()    # acceleration\n",
    "    acc_y = vel_y.diff()    \n",
    "\n",
    "    # Curvature = |v × a| / |v|^3\n",
    "    #  độ cong chuyển động\n",
    "    cross_prod = vel_x * acc_y - vel_y * acc_x\n",
    "    vel_mag = np.sqrt(vel_x**2 + vel_y**2)\n",
    "    curvature = np.abs(cross_prod) / (vel_mag**3 + 1e-6)\n",
    "\n",
    "    for w in [25, 50, 75]:\n",
    "        ws = _scale(w, fps)\n",
    "        X[f'curv_mean_{w}'] = curvature.rolling(ws, min_periods=max(1, ws // 5)).mean()\n",
    "\n",
    "    # Turn rate (angular velocity)\n",
    "    angle = np.arctan2(vel_y, vel_x)\n",
    "    angle_change = np.abs(angle.diff())\n",
    "    w = 30\n",
    "    ws = _scale(w, fps)\n",
    "    X[f'turn_rate_{w}'] = angle_change.rolling(ws, min_periods=max(1, ws // 5)).sum()\n",
    "    # thay đổi góc. sum là kiểu tổng số rad đã xoay trong thời gian đó\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72d7cc40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T01:13:30.175398Z",
     "iopub.status.busy": "2025-12-12T01:13:30.175000Z",
     "iopub.status.idle": "2025-12-12T01:13:30.184637Z",
     "shell.execute_reply": "2025-12-12T01:13:30.183367Z"
    },
    "papermill": {
     "duration": 0.025944,
     "end_time": "2025-12-12T01:13:30.186532",
     "exception": false,
     "start_time": "2025-12-12T01:13:30.160588",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_multiscale_features(X, center_x, center_y, fps):\n",
    "    \"\"\"Analyze speed at multiple time scales\"\"\"\n",
    "    # nhân thêm fps để đổi từ pixel/frame thành đơn vị pixel/s\n",
    "    speed = np.sqrt(center_x.diff()**2 + center_y.diff()**2) * float(fps)\n",
    "\n",
    "    scales = [20, 40, 60, 80]\n",
    "    for scale in scales:\n",
    "        ws = _scale(scale, fps)\n",
    "        if len(speed) >= ws:\n",
    "            X[f'sp_m{scale}'] = speed.rolling(ws, min_periods=max(1, ws // 4)).mean()\n",
    "            X[f'sp_s{scale}'] = speed.rolling(ws, min_periods=max(1, ws // 4)).std()\n",
    "\n",
    "    # Speed ratio (short-term vs long-term)\n",
    "    if len(scales) >= 2 and f'sp_m{scales[0]}' in X.columns and f'sp_m{scales[-1]}' in X.columns:\n",
    "        # nếu ratio > 1 -> tăng tốc đột ngột, còn không thì dạng giảm tốc\n",
    "        X['sp_ratio'] = X[f'sp_m{scales[0]}'] / (X[f'sp_m{scales[-1]}'] + 1e-6)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "955d286a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T01:13:30.220141Z",
     "iopub.status.busy": "2025-12-12T01:13:30.219773Z",
     "iopub.status.idle": "2025-12-12T01:13:30.230389Z",
     "shell.execute_reply": "2025-12-12T01:13:30.229486Z"
    },
    "papermill": {
     "duration": 0.030345,
     "end_time": "2025-12-12T01:13:30.231976",
     "exception": false,
     "start_time": "2025-12-12T01:13:30.201631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_state_features(X, center_x, center_y, fps):\n",
    "    \"\"\"Model movement states (immobile → slow → medium → fast)\"\"\"\n",
    "    speed = np.sqrt(center_x.diff()**2 + center_y.diff()**2) * float(fps)\n",
    "    w_ma = _scale(15, fps)\n",
    "    speed_ma = speed.rolling(w_ma, min_periods=max(1, w_ma // 3)).mean()\n",
    "\n",
    "    try:\n",
    "        # Bin speeds into 4 states\n",
    "        bins = [-np.inf, 0.5 * fps, 2.0 * fps, 5.0 * fps, np.inf]\n",
    "        speed_states = pd.cut(speed_ma, bins=bins, labels=[0, 1, 2, 3]).astype(float)\n",
    "\n",
    "        \"\"\"\n",
    "        df = pd.DataFrame({'Điểm': [45, 78, 92, 55, 88, 60, 70]})\n",
    "\n",
    "        # 2. Định nghĩa các bins và labels\n",
    "        bins = [0, 60, 75, 90, 100]\n",
    "        labels = ['Dưới TB', 'Trung bình', 'Khá', 'Giỏi']\n",
    "\n",
    "        # 3. Áp dụng pd.cut\n",
    "        df['Xếp loại'] = pd.cut(df['Điểm'], bins=bins, labels=labels, right=False) # right=False: khoảng [a, b)\n",
    "\n",
    "        print(df)\n",
    "            Điểm Xếp loại\n",
    "        0    45  Dưới TB\n",
    "        1    78     Khá\n",
    "        2    92    Giỏi\n",
    "        3    55  Dưới TB\n",
    "        4    88     Khá\n",
    "        5    60  Dưới TB\n",
    "        6    70 Trung bình\n",
    "        \"\"\"\n",
    "\n",
    "        for window in [20, 40, 60, 80]:\n",
    "            ws = _scale(window, fps)\n",
    "            if len(speed_states) >= ws:\n",
    "                # tính tần suất xuất hiện của mỗi trạng thái\n",
    "                for state in [0, 1, 2, 3]:\n",
    "                    X[f's{state}_{window}'] = (\n",
    "                        (speed_states == state).astype(float)\n",
    "                        .rolling(ws, min_periods=max(1, ws // 5)).mean()\n",
    "                    )\n",
    "                # số lần đổi trạng thái\n",
    "                state_changes = (speed_states != speed_states.shift(1)).astype(float)\n",
    "                X[f'trans_{window}'] = state_changes.rolling(ws, min_periods=max(1, ws // 5)).sum()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb4d89ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T01:13:30.254959Z",
     "iopub.status.busy": "2025-12-12T01:13:30.254630Z",
     "iopub.status.idle": "2025-12-12T01:13:30.262736Z",
     "shell.execute_reply": "2025-12-12T01:13:30.261650Z"
    },
    "papermill": {
     "duration": 0.021692,
     "end_time": "2025-12-12T01:13:30.264488",
     "exception": false,
     "start_time": "2025-12-12T01:13:30.242796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_longrange_features(X, center_x, center_y, fps):\n",
    "    \"\"\"Long-range rolling means, EMA, and percentile ranks\"\"\"\n",
    "    # Long-range rolling means\n",
    "    for window in [30, 60, 120]:\n",
    "        ws = _scale(window, fps)\n",
    "        if len(center_x) >= ws:\n",
    "            X[f'x_ml{window}'] = center_x.rolling(ws, min_periods=max(5, ws // 6)).mean()\n",
    "            X[f'y_ml{window}'] = center_y.rolling(ws, min_periods=max(5, ws // 6)).mean()\n",
    "\n",
    "    # Exponential moving averages\n",
    "    for span in [30, 60, 120]:\n",
    "        s = _scale(span, fps)\n",
    "        X[f'x_e{span}'] = center_x.ewm(span=s, min_periods=1).mean()\n",
    "        X[f'y_e{span}'] = center_y.ewm(span=s, min_periods=1).mean()\n",
    "\n",
    "    # Speed percentile ranks\n",
    "    speed = np.sqrt(center_x.diff()**2 + center_y.diff()**2) * float(fps)\n",
    "    for window in [30, 60, 120]:\n",
    "        ws = _scale(window, fps)\n",
    "        if len(speed) >= ws:\n",
    "            X[f'sp_pct{window}'] = speed.rolling(ws, min_periods=max(5, ws // 6)).rank(pct=True)\n",
    "        # Tốc độ hiện tại của vật thể cao hơn \n",
    "        # bao nhiêu phần trăm của các tốc độ mà nó đã từng đạt được trong $X$ giây vừa qua?\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67edd6f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T01:13:30.287474Z",
     "iopub.status.busy": "2025-12-12T01:13:30.287192Z",
     "iopub.status.idle": "2025-12-12T01:13:30.297451Z",
     "shell.execute_reply": "2025-12-12T01:13:30.296391Z"
    },
    "papermill": {
     "duration": 0.02358,
     "end_time": "2025-12-12T01:13:30.299009",
     "exception": false,
     "start_time": "2025-12-12T01:13:30.275429",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_interaction_features(X, mouse_pair, avail_A, avail_B, fps):\n",
    "    \"\"\"Advanced interaction features for pairs (leading, chasing, etc.)\"\"\"\n",
    "    if 'body_center' not in avail_A or 'body_center' not in avail_B:\n",
    "        return X\n",
    "\n",
    "    rel_x = mouse_pair['A']['body_center']['x'] - mouse_pair['B']['body_center']['x']\n",
    "    rel_y = mouse_pair['A']['body_center']['y'] - mouse_pair['B']['body_center']['y']\n",
    "    rel_dist = np.sqrt(rel_x**2 + rel_y**2)\n",
    "\n",
    "    A_vx = mouse_pair['A']['body_center']['x'].diff()\n",
    "    A_vy = mouse_pair['A']['body_center']['y'].diff()\n",
    "    B_vx = mouse_pair['B']['body_center']['x'].diff()\n",
    "    B_vy = mouse_pair['B']['body_center']['y'].diff()\n",
    "\n",
    "    # Lead = 1: Con chuột đó đang di chuyển thẳng về phía đối phương.\n",
    "    # Lead = -1: Con chuột đó đang di chuyển thẳng ra xa đối phương.\n",
    "    # Lead = 0: Di chuyển vuông góc với đối phương\n",
    "    A_lead = (A_vx * rel_x + A_vy * rel_y) / (np.sqrt(A_vx**2 + A_vy**2) * rel_dist + 1e-6)\n",
    "    B_lead = (B_vx * (-rel_x) + B_vy * (-rel_y)) / (np.sqrt(B_vx**2 + B_vy**2) * rel_dist + 1e-6)\n",
    "\n",
    "    for window in [30, 60]:\n",
    "        ws = _scale(window, fps)\n",
    "        X[f'A_ld{window}'] = A_lead.rolling(ws, min_periods=max(1, ws // 6)).mean()\n",
    "        X[f'B_ld{window}'] = B_lead.rolling(ws, min_periods=max(1, ws // 6)).mean()\n",
    "\n",
    "    # Chase behavior (approach rate * B_lead)\n",
    "    approach = -rel_dist.diff()\n",
    "    chase = approach * B_lead\n",
    "    # | approach | B_lead | chase     | Ý nghĩa vật lý                             |\n",
    "    # | -------- | ------ | --------- | ------------------------------------------ |\n",
    "    # | +        | +      | + lớn     | B đang đuổi theo A thật sự                 |\n",
    "    # | +        | -      | -         | Khoảng cách giảm nhưng B không hướng vào A |\n",
    "    # | -        | +      | -         | B hướng vào A nhưng khoảng cách vẫn tăng   |\n",
    "    # | -        | -      | +         | Cả hai đều rời xa                          |\n",
    "    # | ≈ 0      | bất kỳ | ≈ 0       | Không có tương tác rõ ràng                 |\n",
    "\n",
    "    w = 30\n",
    "    ws = _scale(w, fps)\n",
    "    X[f'chase_{w}'] = chase.rolling(ws, min_periods=max(1, ws // 6)).mean()\n",
    "\n",
    "    # Speed correlation\n",
    "    for window in [60, 120]:\n",
    "        ws = _scale(window, fps)\n",
    "        A_sp = np.sqrt(A_vx**2 + A_vy**2)\n",
    "        B_sp = np.sqrt(B_vx**2 + B_vy**2)\n",
    "        X[f'sp_cor{window}'] = A_sp.rolling(ws, min_periods=max(1, ws // 6)).corr(B_sp)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29c8e946",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T01:13:30.322020Z",
     "iopub.status.busy": "2025-12-12T01:13:30.321657Z",
     "iopub.status.idle": "2025-12-12T01:13:30.330072Z",
     "shell.execute_reply": "2025-12-12T01:13:30.329105Z"
    },
    "papermill": {
     "duration": 0.021573,
     "end_time": "2025-12-12T01:13:30.331479",
     "exception": false,
     "start_time": "2025-12-12T01:13:30.309906",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_groom_microfeatures(X, df, fps):\n",
    "    parts = df.columns.get_level_values(0)\n",
    "    if 'body_center' not in parts or 'nose' not in parts:\n",
    "        return X\n",
    "\n",
    "    cx = df['body_center']['x']; cy = df['body_center']['y']\n",
    "    nx = df['nose']['x']; ny = df['nose']['y']\n",
    "\n",
    "    # Tốc độ\n",
    "    cs = (np.sqrt(cx.diff()**2 + cy.diff()**2) * float(fps)).fillna(0)\n",
    "    ns = (np.sqrt(nx.diff()**2 + ny.diff()**2) * float(fps)).fillna(0)\n",
    "\n",
    "    w30 = _scale(30, fps)\n",
    "    # rate tốc độ mũi / tốc độ thân (Đầu di chuyển nhiều trong khi thân đứng yên -> grooming)\n",
    "    X['head_body_decouple'] = (ns / (cs + 1e-3)).clip(0, 10).rolling(w30, min_periods=max(1, w30//3)).median()\n",
    "\n",
    "    # bán kính chuyển động của mũi so với tâm\n",
    "    r = np.sqrt((nx - cx)**2 + (ny - cy)**2)\n",
    "    X['nose_rad_std'] = r.rolling(w30, min_periods=max(1, w30//3)).std().fillna(0)\n",
    "\n",
    "    if 'tail_base' in parts:\n",
    "        # độ rung lắc của đầu\n",
    "        ang = np.arctan2(df['nose']['y']-df['tail_base']['y'], df['nose']['x']-df['tail_base']['x'])\n",
    "        dang = np.abs(ang.diff()).fillna(0)\n",
    "        X['head_orient_jitter'] = dang.rolling(w30, min_periods=max(1, w30//3)).mean()\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77a81ac4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T01:13:30.355246Z",
     "iopub.status.busy": "2025-12-12T01:13:30.354575Z",
     "iopub.status.idle": "2025-12-12T01:13:30.361552Z",
     "shell.execute_reply": "2025-12-12T01:13:30.360608Z"
    },
    "papermill": {
     "duration": 0.021105,
     "end_time": "2025-12-12T01:13:30.363393",
     "exception": false,
     "start_time": "2025-12-12T01:13:30.342288",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_speed_asymmetry_future_past_single(\n",
    "    X: pd.DataFrame, cx: pd.Series, cy: pd.Series, fps: float,\n",
    "    horizon_base: int = 30, agg: str = \"mean\"\n",
    ") -> pd.DataFrame:\n",
    "    w = max(3, _scale(horizon_base, fps))\n",
    "    v = _speed(cx, cy, fps)\n",
    "    if agg == \"median\":\n",
    "        v_past = v.rolling(w, min_periods=max(3, w//4), center=False).median()\n",
    "        v_fut  = v.iloc[::-1].rolling(w, min_periods=max(3, w//4)).median().iloc[::-1]\n",
    "    else:\n",
    "        v_past = v.rolling(w, min_periods=max(3, w//4), center=False).mean()\n",
    "        v_fut  = _roll_future_mean(v, w, min_p=max(3, w//4))\n",
    "    X[\"spd_asym_1s\"] = (v_fut - v_past).fillna(0.0)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b51eb8bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T01:13:30.387587Z",
     "iopub.status.busy": "2025-12-12T01:13:30.386953Z",
     "iopub.status.idle": "2025-12-12T01:13:30.394496Z",
     "shell.execute_reply": "2025-12-12T01:13:30.393525Z"
    },
    "papermill": {
     "duration": 0.021543,
     "end_time": "2025-12-12T01:13:30.396141",
     "exception": false,
     "start_time": "2025-12-12T01:13:30.374598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_gauss_shift_speed_future_past_single(X, cx, cy, fps, window_base=30, eps=1e-6):\n",
    "    \"\"\"\n",
    "        độ lệch phân phối tốc độ giữa quá khứ và tương lai\n",
    "        dùng KL divergence để đo sự khác biệt giữa 2 phân phối Gaussian\n",
    "    \"\"\"\n",
    "    w = max(5, _scale(window_base, fps))\n",
    "    v = _speed(cx, cy, fps)\n",
    "\n",
    "    mu_p = v.rolling(w, min_periods=max(3, w//4)).mean()\n",
    "    va_p = v.rolling(w, min_periods=max(3, w//4)).var().clip(lower=eps)\n",
    "\n",
    "    mu_f = _roll_future_mean(v, w, min_p=max(3, w//4))\n",
    "    va_f = _roll_future_var(v, w, min_p=max(3, w//4)).clip(lower=eps)\n",
    "\n",
    "    # KL(Np||Nf) + KL(Nf||Np)\n",
    "    kl_pf = 0.5 * ((va_p/va_f) + ((mu_f - mu_p)**2)/va_f - 1.0 + np.log(va_f/va_p))\n",
    "    kl_fp = 0.5 * ((va_f/va_p) + ((mu_p - mu_f)**2)/va_p - 1.0 + np.log(va_p/va_f))\n",
    "    X[\"spd_symkl_1s\"] = (kl_pf + kl_fp).replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "906a942e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T01:13:30.419470Z",
     "iopub.status.busy": "2025-12-12T01:13:30.418975Z",
     "iopub.status.idle": "2025-12-12T01:13:30.425500Z",
     "shell.execute_reply": "2025-12-12T01:13:30.424243Z"
    },
    "papermill": {
     "duration": 0.020059,
     "end_time": "2025-12-12T01:13:30.427089",
     "exception": false,
     "start_time": "2025-12-12T01:13:30.407030",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_cumulative_distance_single(X, cx, cy, fps, horizon_frames_base=180, colname=\"path_cum180\"):\n",
    "    \"\"\"\n",
    "    tổng quángđường đi trong horizon_frames_base\n",
    "    \"\"\"\n",
    "    L = max(1, _scale(horizon_frames_base, fps))  # frames\n",
    "    # step length (cm per frame since coords are cm)\n",
    "    step = np.hypot(cx.diff(), cy.diff())\n",
    "    # centered rolling sum over ~2L+1 frames (acausal)\n",
    "    path = step.rolling(2*L + 1, min_periods=max(5, L//6), center=True).sum()\n",
    "    X[colname] = path.fillna(0.0).astype(np.float32)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb612f4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T01:13:30.450523Z",
     "iopub.status.busy": "2025-12-12T01:13:30.449783Z",
     "iopub.status.idle": "2025-12-12T01:13:30.457330Z",
     "shell.execute_reply": "2025-12-12T01:13:30.456355Z"
    },
    "papermill": {
     "duration": 0.020745,
     "end_time": "2025-12-12T01:13:30.458785",
     "exception": false,
     "start_time": "2025-12-12T01:13:30.438040",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_social_zones(X, mouse_pair, avail_A, avail_B, fps):\n",
    "    if 'body_center' not in avail_A or 'body_center' not in avail_B:\n",
    "        return X\n",
    "    \n",
    "    dist = np.sqrt(\n",
    "        (mouse_pair['A']['body_center']['x'] - mouse_pair['B']['body_center']['x'])**2 +\n",
    "        (mouse_pair['A']['body_center']['y'] - mouse_pair['B']['body_center']['y'])**2\n",
    "    )\n",
    "    \n",
    "    w30 = _scale(30, fps)\n",
    "    w60 = _scale(60, fps)\n",
    "    \n",
    "    # 1-2s, make sure not just come across\n",
    "    X['contact_time'] = (dist < 3).astype(float).rolling(w30, min_periods=5).mean()\n",
    "    X['close_time'] = ((dist >= 3) & (dist < 10)).astype(float).rolling(w30, min_periods=5).mean()\n",
    "    X['social_time'] = ((dist >= 10) & (dist < 30)).astype(float).rolling(w30, min_periods=5).mean()\n",
    "\n",
    "    # zone transition rate (thay đổi zone nhiều = dynamic interaction)\n",
    "    zone = pd.cut(dist, bins=[0, 3, 10, 30, np.inf], labels=[0, 1, 2, 3])\n",
    "    zone_changes = (zone != zone.shift(1)).astype(float)\n",
    "    X['zone_transitions'] = zone_changes.rolling(w60, min_periods=10).sum()\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d703e13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T01:13:30.482776Z",
     "iopub.status.busy": "2025-12-12T01:13:30.482484Z",
     "iopub.status.idle": "2025-12-12T01:13:30.489759Z",
     "shell.execute_reply": "2025-12-12T01:13:30.488786Z"
    },
    "papermill": {
     "duration": 0.020959,
     "end_time": "2025-12-12T01:13:30.491445",
     "exception": false,
     "start_time": "2025-12-12T01:13:30.470486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_facing_feature(X, mouse_pair, avail_A, avail_B):\n",
    "    \"\"\"\n",
    "    Thêm feature xem mouse A có facing mouse B\n",
    "    \"\"\"\n",
    "    if not all(p in avail_A for p in ['nose', 'body_center']) or 'body_center' not in avail_B:\n",
    "        return X\n",
    "    \n",
    "    # Vector hướng đầu của A (từ thân đến mũi)\n",
    "    head_vec_x = mouse_pair['A']['nose']['x'] - mouse_pair['A']['body_center']['x']\n",
    "    head_vec_y = mouse_pair['A']['nose']['y'] - mouse_pair['A']['body_center']['y']\n",
    "    \n",
    "    # Vector từ A đến B\n",
    "    target_vec_x = mouse_pair['B']['body_center']['x'] - mouse_pair['A']['body_center']['x']\n",
    "    target_vec_y = mouse_pair['B']['body_center']['y'] - mouse_pair['A']['body_center']['y']\n",
    "    \n",
    "    # Cosine Similarity\n",
    "    dot = head_vec_x * target_vec_x + head_vec_y * target_vec_y\n",
    "    mag_head = np.sqrt(head_vec_x**2 + head_vec_y**2)\n",
    "    mag_target = np.sqrt(target_vec_x**2 + target_vec_y**2) # -> 1?\n",
    "    \n",
    "    X['A_facing_B'] = dot / (mag_head * mag_target + 1e-6)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8bdf5b20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T01:13:30.515442Z",
     "iopub.status.busy": "2025-12-12T01:13:30.515111Z",
     "iopub.status.idle": "2025-12-12T01:13:30.523176Z",
     "shell.execute_reply": "2025-12-12T01:13:30.522310Z"
    },
    "papermill": {
     "duration": 0.021785,
     "end_time": "2025-12-12T01:13:30.524531",
     "exception": false,
     "start_time": "2025-12-12T01:13:30.502746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_approach_velocity(X, mouse_pair, avail_A, avail_B, fps):\n",
    "    \"\"\"\n",
    "    Maybe support the approach feature above\n",
    "    \"\"\"\n",
    "    if 'body_center' not in avail_A or 'body_center' not in avail_B:\n",
    "        return X\n",
    "    \n",
    "    dist = np.sqrt(\n",
    "        (mouse_pair['A']['body_center']['x'] - mouse_pair['B']['body_center']['x'])**2 +\n",
    "        (mouse_pair['A']['body_center']['y'] - mouse_pair['B']['body_center']['y'])**2\n",
    "    )\n",
    "    \n",
    "    # Approach rate (add - to make sure approach = positive)\n",
    "    approach_rate = -dist.diff() * float(fps)  # cm/s\n",
    "    \n",
    "    w15 = _scale(15, fps)\n",
    "    w60 = _scale(60, fps)\n",
    "    \n",
    "    X['approach_vel_short'] = approach_rate.rolling(w15, min_periods=3).mean()\n",
    "    X['approach_vel_long'] = approach_rate.rolling(w60, min_periods=10).mean()\n",
    "    X['approach_accel'] = approach_rate.diff() * float(fps)\n",
    "    \n",
    "    is_fast = (approach_rate > 5.0).astype(float)\n",
    "    X['fast_approach_frac'] = is_fast.rolling(w60, min_periods=10).mean()\n",
    "\n",
    "    is_slow = ((approach_rate >= 1.0) & (approach_rate <= 5.0)).astype(float)\n",
    "    X['slow_approach_frac'] = is_slow.rolling(w60, min_periods=10).mean()\n",
    "    \n",
    "    is_flee = (approach_rate < -5.0).astype(float)\n",
    "    X['flee_frac'] = is_flee.rolling(w60, min_periods=10).mean()\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bc8567",
   "metadata": {
    "papermill": {
     "duration": 0.010732,
     "end_time": "2025-12-12T01:13:30.546650",
     "exception": false,
     "start_time": "2025-12-12T01:13:30.535918",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature Engineering with FPS Scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c7370e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T01:13:30.569662Z",
     "iopub.status.busy": "2025-12-12T01:13:30.569363Z",
     "iopub.status.idle": "2025-12-12T01:13:30.589870Z",
     "shell.execute_reply": "2025-12-12T01:13:30.588901Z"
    },
    "papermill": {
     "duration": 0.034133,
     "end_time": "2025-12-12T01:13:30.591516",
     "exception": false,
     "start_time": "2025-12-12T01:13:30.557383",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transform_single(single_mouse, body_parts_tracked, fps):\n",
    "    available_body_parts = single_mouse.columns.get_level_values(0)\n",
    "\n",
    "    # Tạo các feature khoảng cách giữa mọi cặp body-parts\n",
    "    X = pd.DataFrame({\n",
    "        f\"{p1}+{p2}\": np.square(single_mouse[p1] - single_mouse[p2]).sum(axis=1, skipna=False)\n",
    "        for p1, p2 in itertools.combinations(body_parts_tracked, 2)\n",
    "        if p1 in available_body_parts and p2 in available_body_parts\n",
    "    })\n",
    "    X = X.reindex(columns=[f\"{p1}+{p2}\" for p1, p2 in itertools.combinations(body_parts_tracked, 2)], copy=False)\n",
    "\n",
    "    # Tốc độ/độ dịch chuyển của một số điểm (ear_left, ear_right, tail_base)\n",
    "    if all(p in single_mouse.columns for p in ['ear_left', 'ear_right', 'tail_base']):\n",
    "        lag = _scale(10, fps)\n",
    "        shifted = single_mouse[['ear_left', 'ear_right', 'tail_base']].shift(lag)\n",
    "        speeds = pd.DataFrame({\n",
    "            'sp_lf': np.square(single_mouse['ear_left'] - shifted['ear_left']).sum(axis=1, skipna=False),\n",
    "            'sp_rt': np.square(single_mouse['ear_right'] - shifted['ear_right']).sum(axis=1, skipna=False),\n",
    "            'sp_lf2': np.square(single_mouse['ear_left'] - shifted['tail_base']).sum(axis=1, skipna=False),\n",
    "            'sp_rt2': np.square(single_mouse['ear_right'] - shifted['tail_base']).sum(axis=1, skipna=False),\n",
    "        })\n",
    "        X = pd.concat([X, speeds], axis=1)\n",
    "\n",
    "    # Tỉ lệ kéo dãn cơ thể (elong)\n",
    "    if 'nose+tail_base' in X.columns and 'ear_left+ear_right' in X.columns:\n",
    "        X['elong'] = X['nose+tail_base'] / (X['ear_left+ear_right'] + 1e-6)\n",
    "\n",
    "    # Góc giữa mũi –trung tâm cơ thể – cuối đuôi (cosine)\n",
    "    if all(p in available_body_parts for p in ['nose', 'body_center', 'tail_base']):\n",
    "        v1 = single_mouse['nose'] - single_mouse['body_center']\n",
    "        v2 = single_mouse['tail_base'] - single_mouse['body_center']\n",
    "        X['body_ang'] = (v1['x'] * v2['x'] + v1['y'] * v2['y']) / (\n",
    "            np.sqrt(v1['x']**2 + v1['y']**2) * np.sqrt(v2['x']**2 + v2['y']**2) + 1e-6)\n",
    "\n",
    "    if 'body_center' in available_body_parts:\n",
    "        cx = single_mouse['body_center']['x']\n",
    "        cy = single_mouse['body_center']['y']\n",
    "\n",
    "        for w in [5, 15, 30, 60]:\n",
    "            ws = _scale(w, fps)\n",
    "            # Tính cả trước và sau + min 1 ô là đủ để tính\n",
    "            roll = dict(min_periods=1, center=True)\n",
    "            # hướng di chuyển trung bình gần đây (mean)\n",
    "            X[f'cx_m{w}'] = cx.rolling(ws, **roll).mean()\n",
    "            X[f'cy_m{w}'] = cy.rolling(ws, **roll).mean()\n",
    "            # chuột có chạy loạn hay đứng yên (std)\n",
    "            X[f'cx_s{w}'] = cx.rolling(ws, **roll).std()\n",
    "            X[f'cy_s{w}'] = cy.rolling(ws, **roll).std()\n",
    "            # chuột di chuyển rộng hay hẹp theo phương x/y (Biên độ dao động)\n",
    "            X[f'x_rng{w}'] = cx.rolling(ws, **roll).max() - cx.rolling(ws, **roll).min()\n",
    "            X[f'y_rng{w}'] = cy.rolling(ws, **roll).max() - cy.rolling(ws, **roll).min()\n",
    "            # độ dài vector dịch chuyển từ đầu đến cuối cửa sổ\n",
    "            X[f'disp{w}'] = np.sqrt(cx.diff().rolling(ws, min_periods=1).sum()**2 +\n",
    "                                     cy.diff().rolling(ws, min_periods=1).sum()**2)\n",
    "            # độ biến động của vận tốc tức thời trong cửa sổ\n",
    "            X[f'act{w}'] = np.sqrt(cx.diff().rolling(ws, min_periods=1).var() +\n",
    "                                   cy.diff().rolling(ws, min_periods=1).var())\n",
    "\n",
    "        # Đo độ cong quỹ đạo, tốc độ đổi hướng: Curvature cao → chuyển động cong mạnh → quay đầu hoặc đổi hướng đột ngột\n",
    "        # Thêm cả Turn rate\n",
    "        X = add_curvature_features(X, cx, cy, fps)\n",
    "        # Phân tích tốc độ ở nhiều thời gian khác nhau\n",
    "        X = add_multiscale_features(X, cx, cy, fps)\n",
    "        # Mô hình hoá trạng thái chuyển động (immobile → slow → medium → fast)\n",
    "        X = add_state_features(X, cx, cy, fps)\n",
    "        # Long-range mean/EMA + Percentile speed\n",
    "        X = add_longrange_features(X, cx, cy, fps)\n",
    "        X = add_cumulative_distance_single(X, cx, cy, fps, horizon_frames_base=180)\n",
    "        X = add_groom_microfeatures(X, single_mouse, fps)\n",
    "        X = add_speed_asymmetry_future_past_single(X, cx, cy, fps, horizon_base=30)         \n",
    "        X = add_gauss_shift_speed_future_past_single(X, cx, cy, fps, window_base=30)\n",
    "\n",
    "    if all(p in available_body_parts for p in ['nose', 'tail_base']):\n",
    "        nt_dist = np.sqrt((single_mouse['nose']['x'] - single_mouse['tail_base']['x'])**2 +\n",
    "                          (single_mouse['nose']['y'] - single_mouse['tail_base']['y'])**2)\n",
    "        for lag in [10, 20, 40]:\n",
    "            l = _scale(lag, fps)\n",
    "            X[f'nt_lg{lag}'] = nt_dist.shift(l)\n",
    "            X[f'nt_df{lag}'] = nt_dist - nt_dist.shift(l)\n",
    "\n",
    "    if all(p in available_body_parts for p in ['ear_left', 'ear_right']):\n",
    "        ear_d = np.sqrt((single_mouse['ear_left']['x'] - single_mouse['ear_right']['x'])**2 +\n",
    "                        (single_mouse['ear_left']['y'] - single_mouse['ear_right']['y'])**2)\n",
    "        for off in [-30, -20, -10, 10, 20, 30]:\n",
    "            o = _scale_signed(off, fps)\n",
    "            X[f'ear_o{off}'] = ear_d.shift(-o)\n",
    "        w = _scale(30, fps)\n",
    "        X['ear_con'] = ear_d.rolling(w, min_periods=1, center=True).std() / \\\n",
    "                       (ear_d.rolling(w, min_periods=1, center=True).mean() + 1e-6)\n",
    "\n",
    "        \n",
    "    return X.astype(np.float32, copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b040b6ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T01:13:30.615719Z",
     "iopub.status.busy": "2025-12-12T01:13:30.615437Z",
     "iopub.status.idle": "2025-12-12T01:13:30.637932Z",
     "shell.execute_reply": "2025-12-12T01:13:30.637061Z"
    },
    "papermill": {
     "duration": 0.03647,
     "end_time": "2025-12-12T01:13:30.639538",
     "exception": false,
     "start_time": "2025-12-12T01:13:30.603068",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transform_pair(mouse_pair, body_parts_tracked, fps):\n",
    "    avail_A = mouse_pair['A'].columns.get_level_values(0)\n",
    "    avail_B = mouse_pair['B'].columns.get_level_values(0)\n",
    "\n",
    "    X = pd.DataFrame({\n",
    "        f\"12+{p1}+{p2}\": np.square(mouse_pair['A'][p1] - mouse_pair['B'][p2]).sum(axis=1, skipna=False)\n",
    "        for p1, p2 in itertools.product(body_parts_tracked, repeat=2)\n",
    "        if p1 in avail_A and p2 in avail_B\n",
    "    })\n",
    "    X = X.reindex(columns=[f\"12+{p1}+{p2}\" for p1, p2 in itertools.product(body_parts_tracked, repeat=2)], copy=False)\n",
    "\n",
    "    if ('A', 'ear_left') in mouse_pair.columns and ('B', 'ear_left') in mouse_pair.columns:\n",
    "        lag = _scale(10, fps)\n",
    "        shA = mouse_pair['A']['ear_left'].shift(lag)\n",
    "        shB = mouse_pair['B']['ear_left'].shift(lag)\n",
    "        speeds = pd.DataFrame({\n",
    "            'sp_A': np.square(mouse_pair['A']['ear_left'] - shA).sum(axis=1, skipna=False),\n",
    "            'sp_AB': np.square(mouse_pair['A']['ear_left'] - shB).sum(axis=1, skipna=False),\n",
    "            'sp_B': np.square(mouse_pair['B']['ear_left'] - shB).sum(axis=1, skipna=False),\n",
    "        })\n",
    "        X = pd.concat([X, speeds], axis=1)\n",
    "\n",
    "    if 'nose+tail_base' in X.columns and 'ear_left+ear_right' in X.columns:\n",
    "        X['elong'] = X['nose+tail_base'] / (X['ear_left+ear_right'] + 1e-6)\n",
    "\n",
    "    if all(p in avail_A for p in ['nose', 'tail_base']) and all(p in avail_B for p in ['nose', 'tail_base']):\n",
    "        dir_A = mouse_pair['A']['nose'] - mouse_pair['A']['tail_base']\n",
    "        dir_B = mouse_pair['B']['nose'] - mouse_pair['B']['tail_base']\n",
    "        X['rel_ori'] = (dir_A['x'] * dir_B['x'] + dir_A['y'] * dir_B['y']) / (\n",
    "            np.sqrt(dir_A['x']**2 + dir_A['y']**2) * np.sqrt(dir_B['x']**2 + dir_B['y']**2) + 1e-6)\n",
    "\n",
    "    if all(p in avail_A for p in ['nose']) and all(p in avail_B for p in ['nose']):\n",
    "        cur = np.square(mouse_pair['A']['nose'] - mouse_pair['B']['nose']).sum(axis=1, skipna=False)\n",
    "        lag = _scale(10, fps)\n",
    "        shA_n = mouse_pair['A']['nose'].shift(lag)\n",
    "        shB_n = mouse_pair['B']['nose'].shift(lag)\n",
    "        past = np.square(shA_n - shB_n).sum(axis=1, skipna=False)\n",
    "        X['appr'] = cur - past\n",
    "\n",
    "    if 'body_center' in avail_A and 'body_center' in avail_B:\n",
    "        cd = np.sqrt((mouse_pair['A']['body_center']['x'] - mouse_pair['B']['body_center']['x'])**2 +\n",
    "                     (mouse_pair['A']['body_center']['y'] - mouse_pair['B']['body_center']['y'])**2)\n",
    "        X['v_cls'] = (cd < 5.0).astype(float)\n",
    "        X['cls']   = ((cd >= 5.0) & (cd < 15.0)).astype(float)\n",
    "        X['med']   = ((cd >= 15.0) & (cd < 30.0)).astype(float)\n",
    "        X['far']   = (cd >= 30.0).astype(float)\n",
    "\n",
    "    if 'body_center' in avail_A and 'body_center' in avail_B:\n",
    "        cd_full = np.square(mouse_pair['A']['body_center'] - mouse_pair['B']['body_center']).sum(axis=1, skipna=False)\n",
    "\n",
    "        for w in [5, 15, 30, 60]:\n",
    "            ws = _scale(w, fps)\n",
    "            roll = dict(min_periods=1, center=True)\n",
    "            X[f'd_m{w}']  = cd_full.rolling(ws, **roll).mean()\n",
    "            X[f'd_s{w}']  = cd_full.rolling(ws, **roll).std()\n",
    "            X[f'd_mn{w}'] = cd_full.rolling(ws, **roll).min()\n",
    "            X[f'd_mx{w}'] = cd_full.rolling(ws, **roll).max()\n",
    "\n",
    "            d_var = cd_full.rolling(ws, **roll).var()\n",
    "            X[f'int{w}'] = 1 / (1 + d_var)\n",
    "\n",
    "            Axd = mouse_pair['A']['body_center']['x'].diff()\n",
    "            Ayd = mouse_pair['A']['body_center']['y'].diff()\n",
    "            Bxd = mouse_pair['B']['body_center']['x'].diff()\n",
    "            Byd = mouse_pair['B']['body_center']['y'].diff()\n",
    "            coord = Axd * Bxd + Ayd * Byd\n",
    "            X[f'co_m{w}'] = coord.rolling(ws, **roll).mean()\n",
    "            X[f'co_s{w}'] = coord.rolling(ws, **roll).std()\n",
    "\n",
    "    if 'nose' in avail_A and 'nose' in avail_B:\n",
    "        nn = np.sqrt((mouse_pair['A']['nose']['x'] - mouse_pair['B']['nose']['x'])**2 +\n",
    "                     (mouse_pair['A']['nose']['y'] - mouse_pair['B']['nose']['y'])**2)\n",
    "        for lag in [10, 20, 40]:\n",
    "            l = _scale(lag, fps)\n",
    "            X[f'nn_lg{lag}']  = nn.shift(l)\n",
    "            X[f'nn_ch{lag}']  = nn - nn.shift(l)\n",
    "            is_cl = (nn < 10.0).astype(float)\n",
    "            X[f'cl_ps{lag}']  = is_cl.rolling(l, min_periods=1).mean()\n",
    "\n",
    "    if 'body_center' in avail_A and 'body_center' in avail_B:\n",
    "        Avx = mouse_pair['A']['body_center']['x'].diff()\n",
    "        Avy = mouse_pair['A']['body_center']['y'].diff()\n",
    "        Bvx = mouse_pair['B']['body_center']['x'].diff()\n",
    "        Bvy = mouse_pair['B']['body_center']['y'].diff()\n",
    "        val = (Avx * Bvx + Avy * Bvy) / (np.sqrt(Avx**2 + Avy**2) * np.sqrt(Bvx**2 + Bvy**2) + 1e-6)\n",
    "\n",
    "        for off in [-30, -20, -10, 0, 10, 20, 30]:\n",
    "            o = _scale_signed(off, fps)\n",
    "            X[f'va_{off}'] = val.shift(-o)\n",
    "\n",
    "        w = _scale(30, fps)\n",
    "        X['int_con'] = cd_full.rolling(w, min_periods=1, center=True).std() / \\\n",
    "                       (cd_full.rolling(w, min_periods=1, center=True).mean() + 1e-6)\n",
    "\n",
    "        X = add_interaction_features(X, mouse_pair, avail_A, avail_B, fps)\n",
    "        X = add_facing_feature(X, mouse_pair, avail_A, avail_B)\n",
    "        X = add_social_zones(X, mouse_pair, avail_A, avail_B, fps)\n",
    "        X = add_approach_velocity(X, mouse_pair, avail_A, avail_B, fps)\n",
    "\n",
    "    return X.astype(np.float32, copy=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6989a93",
   "metadata": {
    "papermill": {
     "duration": 0.011068,
     "end_time": "2025-12-12T01:13:30.662010",
     "exception": false,
     "start_time": "2025-12-12T01:13:30.650942",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Threshold Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3e0b619",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T01:13:30.686547Z",
     "iopub.status.busy": "2025-12-12T01:13:30.686220Z",
     "iopub.status.idle": "2025-12-12T01:13:30.696962Z",
     "shell.execute_reply": "2025-12-12T01:13:30.696025Z"
    },
    "papermill": {
     "duration": 0.025109,
     "end_time": "2025-12-12T01:13:30.698399",
     "exception": false,
     "start_time": "2025-12-12T01:13:30.673290",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_short_events_vectorized(binary_pred, min_duration):\n",
    "    \"\"\"\n",
    "    Hàm lọc bỏ các sự kiện ngắn (short events) trong chuỗi nhị phân. \n",
    "    Tại cơ bản nó là nhiễu\n",
    "    \"\"\"\n",
    "    if min_duration <= 1:\n",
    "        return binary_pred\n",
    "\n",
    "    structure = np.ones(3, dtype=int) \n",
    "    # gán nhán cho 1\n",
    "    # kiểu [0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0] -> [0, 0, 1, 1, 0, 2, 0, 0, 3, 3, 3, 0]\n",
    "    # 0 thì vẫn giữ nguyên\n",
    "    labeled, n_features = scipy_label(binary_pred, structure=structure)\n",
    "    \n",
    "    # Đếm số ptu mỗi nhóm\n",
    "    counts = np.bincount(labeled)\n",
    "    \n",
    "    remove_labels = np.where((counts < min_duration) & (np.arange(len(counts)) > 0))[0]\n",
    "    \n",
    "    mask = np.isin(labeled, remove_labels)\n",
    "    binary_pred[mask] = 0\n",
    "    \n",
    "    # trả về mask, true ở chỗ noise\n",
    "    return binary_pred\n",
    "\n",
    "\n",
    "def gap_filling(arr, empty_val, limit=2):\n",
    "    \"\"\"\n",
    "    ffill và bfill với limit nhất định\n",
    "    \"\"\"\n",
    "    arr = arr.copy()\n",
    "\n",
    "    for _ in range(limit):\n",
    "        # tạo mask vị trí 0\n",
    "        is_empty = (arr == empty_val)\n",
    "        fill_mask = is_empty[1:] & (arr[:-1] != empty_val)\n",
    "        arr[1:][fill_mask] = arr[:-1][fill_mask]\n",
    "\n",
    "    arr = arr[::-1]\n",
    "    for _ in range(limit):\n",
    "        is_empty = (arr == empty_val)\n",
    "        fill_mask = is_empty[1:] & (arr[:-1] != empty_val)\n",
    "        arr[1:][fill_mask] = arr[:-1][fill_mask]\n",
    "    \n",
    "    return arr[::-1]\n",
    "\n",
    "def tune_threshold_optuna_advanced(oof_action, y_action, n_trials=100):\n",
    "\n",
    "    # temperal smoothing -> giảm noise\n",
    "    oof_series = pd.Series(oof_action)\n",
    "    oof_smoothed = oof_series.rolling(window=5, min_periods=1, center=True).mean().values\n",
    "    \n",
    "    y_true = np.array(y_action, dtype=int)\n",
    "\n",
    "    def objective(trial):\n",
    "        threshold = trial.suggest_float(\"threshold\", 0.1, 0.9, step=0.01)\n",
    "        min_duration = trial.suggest_int(\"min_duration\", 1, 10) \n",
    "\n",
    "        # apply threshold\n",
    "        binary_pred = (oof_smoothed >= threshold).astype(int)\n",
    "\n",
    "        # gap filling (forward + backward fill)\n",
    "        binary_filled = gap_filling(binary_pred, empty_val=0, limit=2)\n",
    "        \n",
    "        # filter short events\n",
    "        final_pred = filter_short_events_vectorized(binary_filled.copy(), min_duration)\n",
    "        \n",
    "        return f1_score(y_true, final_pred, zero_division=0)\n",
    "\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "    \n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=n_trials, n_jobs=-1)\n",
    "    \n",
    "    # print(f\"  -> Best F1: {study.best_value:.4f} | Thresh: {study.best_params['threshold']:.2f} | Min Dur: {study.best_params['min_duration']}\")\n",
    "    \n",
    "    # Trả về cả 2 tham số tốt nhất\n",
    "    return study.best_params[\"threshold\"], study.best_params[\"min_duration\"], study.best_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e76bff",
   "metadata": {
    "papermill": {
     "duration": 0.010702,
     "end_time": "2025-12-12T01:13:30.720065",
     "exception": false,
     "start_time": "2025-12-12T01:13:30.709363",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Robustify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf425b37",
   "metadata": {
    "_kg_hide-input": false,
    "execution": {
     "iopub.execute_input": "2025-12-12T01:13:30.743607Z",
     "iopub.status.busy": "2025-12-12T01:13:30.743306Z",
     "iopub.status.idle": "2025-12-12T01:13:30.755167Z",
     "shell.execute_reply": "2025-12-12T01:13:30.754248Z"
    },
    "papermill": {
     "duration": 0.025657,
     "end_time": "2025-12-12T01:13:30.756584",
     "exception": false,
     "start_time": "2025-12-12T01:13:30.730927",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def robustify(submission, dataset, traintest, traintest_directory=None):\n",
    "    if traintest_directory is None:\n",
    "        traintest_directory = f\"/kaggle/input/MABe-mouse-behavior-detection/{traintest}_tracking\"\n",
    "\n",
    "    old_submission = submission.copy()\n",
    "    submission = submission[submission.start_frame < submission.stop_frame]\n",
    "    if len(submission) != len(old_submission):\n",
    "        print(\"ERROR: Dropped frames with start >= stop\")\n",
    "    \n",
    "    old_submission = submission.copy()\n",
    "    group_list = []\n",
    "    for _, group in submission.groupby(['video_id', 'agent_id', 'target_id']):\n",
    "        group = group.sort_values('start_frame')\n",
    "        mask = np.ones(len(group), dtype=bool)\n",
    "        last_stop_frame = 0\n",
    "        for i, (_, row) in enumerate(group.iterrows()):\n",
    "            if row['start_frame'] < last_stop_frame:\n",
    "                mask[i] = False\n",
    "            else:\n",
    "                last_stop_frame = row['stop_frame']\n",
    "        group_list.append(group[mask])\n",
    "        \n",
    "    submission = pd.concat(group_list)\n",
    "    \n",
    "    if len(submission) != len(old_submission):\n",
    "        print(\"ERROR: Dropped duplicate frames\")\n",
    "        \n",
    "    s_list = []\n",
    "    for idx, row in dataset.iterrows():\n",
    "        lab_id = row['lab_id']\n",
    "        if lab_id.startswith('MABe22'):\n",
    "            continue\n",
    "        \n",
    "        video_id = row['video_id']\n",
    "        if (submission.video_id == video_id).any():\n",
    "            continue\n",
    "        \n",
    "        if type(row.behaviors_labeled) != str:\n",
    "            continue\n",
    "\n",
    "        print(f\"Video {video_id} has no predictions.\")\n",
    "        \n",
    "        path = f\"{traintest_directory}/{lab_id}/{video_id}.parquet\"\n",
    "        vid = pd.read_parquet(path)\n",
    "    \n",
    "        vid_behaviors = json.loads(row['behaviors_labeled'])\n",
    "        vid_behaviors = sorted(list({b.replace(\"'\", \"\") for b in vid_behaviors}))\n",
    "        vid_behaviors = [b.split(',') for b in vid_behaviors]\n",
    "        vid_behaviors = pd.DataFrame(vid_behaviors, columns=['agent', 'target', 'action'])\n",
    "    \n",
    "        start_frame = vid.video_frame.min()\n",
    "        stop_frame = vid.video_frame.max() + 1\n",
    "    \n",
    "        for (agent, target), actions in vid_behaviors.groupby(['agent', 'target']):\n",
    "            batch_length = int(np.ceil((stop_frame - start_frame) / len(actions)))\n",
    "            for i, (_, action_row) in enumerate(actions.iterrows()):\n",
    "                batch_start = start_frame + i * batch_length\n",
    "                batch_stop = min(batch_start + batch_length, stop_frame)\n",
    "                s_list.append((video_id, agent, target, action_row['action'], batch_start, batch_stop))\n",
    "\n",
    "    if len(s_list) > 0:\n",
    "        submission = pd.concat([\n",
    "            submission,\n",
    "            pd.DataFrame(s_list, columns=['video_id', 'agent_id', 'target_id', 'action', 'start_frame', 'stop_frame'])\n",
    "        ])\n",
    "        print(\"ERROR: Filled empty videos\")\n",
    "\n",
    "    submission = submission.reset_index(drop=True)\n",
    "    \n",
    "    return submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff2bedc",
   "metadata": {
    "papermill": {
     "duration": 0.01113,
     "end_time": "2025-12-12T01:13:30.778895",
     "exception": false,
     "start_time": "2025-12-12T01:13:30.767765",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Multiclass Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f184de6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T01:13:30.802078Z",
     "iopub.status.busy": "2025-12-12T01:13:30.801719Z",
     "iopub.status.idle": "2025-12-12T01:13:30.813498Z",
     "shell.execute_reply": "2025-12-12T01:13:30.812321Z"
    },
    "papermill": {
     "duration": 0.025366,
     "end_time": "2025-12-12T01:13:30.815034",
     "exception": false,
     "start_time": "2025-12-12T01:13:30.789668",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_multiclass(pred, meta, thresholds, min_durations):\n",
    "    \n",
    "    # temporal smoothing\n",
    "    pred_smoothed = pred.rolling(window=5, min_periods=1, center=True).mean()\n",
    "\n",
    "    ama = np.argmax(pred_smoothed.values, axis=1)\n",
    "    max_proba = pred_smoothed.max(axis=1).values\n",
    "\n",
    "    threshold_array = np.array([thresholds.get(col, 0.27) for col in pred_smoothed.columns])\n",
    "    action_thresholds = threshold_array[ama]\n",
    "\n",
    "    ama = np.where(max_proba >= action_thresholds, ama, -1)\n",
    "    # gap filling\n",
    "    ama_filled = gap_filling(ama, empty_val=-1, limit=2)\n",
    "    ama = pd.Series(ama_filled, index=meta.video_frame)\n",
    "\n",
    "    changes_mask = (ama != ama.shift(1)).values\n",
    "    ama_changes = ama[changes_mask]\n",
    "    meta_changes = meta[changes_mask]\n",
    "    \n",
    "    mask = ama_changes.values >= 0\n",
    "    mask[-1] = False\n",
    "    \n",
    "    submission_part = pd.DataFrame({\n",
    "        'video_id': meta_changes['video_id'][mask].values,\n",
    "        'agent_id': meta_changes['agent_id'][mask].values,\n",
    "        'target_id': meta_changes['target_id'][mask].values,\n",
    "        'action': pred.columns[ama_changes[mask].values],\n",
    "        'start_frame': ama_changes.index[mask],\n",
    "        'stop_frame': ama_changes.index[1:][mask[:-1]]\n",
    "    })\n",
    "    \n",
    "    stop_video_id = meta_changes['video_id'][1:][mask[:-1]].values\n",
    "    stop_agent_id = meta_changes['agent_id'][1:][mask[:-1]].values\n",
    "    stop_target_id = meta_changes['target_id'][1:][mask[:-1]].values\n",
    "    for i in range(len(submission_part)):\n",
    "        video_id = submission_part.video_id.iloc[i]\n",
    "        agent_id = submission_part.agent_id.iloc[i]\n",
    "        target_id = submission_part.target_id.iloc[i]\n",
    "        if stop_video_id[i] != video_id or stop_agent_id[i] != agent_id or stop_target_id[i] != target_id:\n",
    "            new_stop_frame = meta.query(\"(video_id == @video_id)\").video_frame.max() + 1\n",
    "            submission_part.iat[i, submission_part.columns.get_loc('stop_frame')] = new_stop_frame\n",
    "\n",
    "    # filter short events\n",
    "    filtered_rows = []\n",
    "    for idx, row in submission_part.iterrows():\n",
    "        action = row['action']\n",
    "        duration = row['stop_frame'] - row['start_frame']\n",
    "        min_dur = min_durations.get(action, 3)  # default = 3\n",
    "        if duration >= min_dur:\n",
    "            filtered_rows.append(row)\n",
    "    \n",
    "    if filtered_rows:\n",
    "        submission_part = pd.DataFrame(filtered_rows).reset_index(drop=True)\n",
    "    else:\n",
    "        submission_part = pd.DataFrame(columns=submission_part.columns)\n",
    "\n",
    "    return submission_part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff998a0e",
   "metadata": {
    "papermill": {
     "duration": 0.011131,
     "end_time": "2025-12-12T01:13:30.837375",
     "exception": false,
     "start_time": "2025-12-12T01:13:30.826244",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training and Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "29404dea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T01:13:30.861253Z",
     "iopub.status.busy": "2025-12-12T01:13:30.860802Z",
     "iopub.status.idle": "2025-12-12T01:13:30.871814Z",
     "shell.execute_reply": "2025-12-12T01:13:30.870770Z"
    },
    "papermill": {
     "duration": 0.0249,
     "end_time": "2025-12-12T01:13:30.873361",
     "exception": false,
     "start_time": "2025-12-12T01:13:30.848461",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cross_validate_classifier(X, label, meta, body_parts_tracked_str, section):\n",
    "    oof = pd.DataFrame(index=meta.video_frame)\n",
    "    \n",
    "    f1_list = []\n",
    "    submission_list = []\n",
    "    thresholds = {}\n",
    "    min_durations = {}\n",
    "    \n",
    "    for action in label.columns:\n",
    "        action_mask = ~ label[action].isna().values\n",
    "        y_action = label[action][action_mask].values.astype(int)\n",
    "        X_action = X[action_mask]\n",
    "        groups_action = meta.video_id[action_mask]\n",
    "        \n",
    "        if len(np.unique(groups_action)) < CFG.n_splits:\n",
    "            continue\n",
    "\n",
    "        if not (y_action == 0).all():\n",
    "            try:\n",
    "                save_path_full = f\"{CFG.model_path}/{CFG.model_name}/{section}/{action}\"\n",
    "\n",
    "                trainer = Trainer(\n",
    "                    estimator=clone(CFG.model),\n",
    "                    cv=CFG.cv,\n",
    "                    cv_args={\"groups\": groups_action},\n",
    "                    metric=f1_score,\n",
    "                    task=\"binary\",\n",
    "                    verbose=False,\n",
    "                    save=True,  # Save fold models\n",
    "                    save_path=save_path_full\n",
    "                )\n",
    "                \n",
    "                trainer.fit(X_action, y_action)\n",
    "                oof_action = trainer.oof_preds  # OOF predictions từ Koolbox\n",
    "                \n",
    "                # Tune threshold trên OOF predictions\n",
    "                threshold, min_duration, f1 = tune_threshold_optuna_advanced(oof_action, y_action)\n",
    "                thresholds[action] = threshold\n",
    "                min_durations[action] = min_duration\n",
    "            \n",
    "                f1_list.append((body_parts_tracked_str, action, f1, threshold))\n",
    "                print(f\"\\tF1: {f1:.4f} (thresh={threshold:.2f}) Section: {section} Action: {action}\")\n",
    "                \n",
    "                # Save threshold riêng\n",
    "                os.makedirs(save_path_full, exist_ok=True)\n",
    "                joblib.dump(threshold, f\"{save_path_full}/threshold.pkl\")\n",
    "                joblib.dump(min_duration, f\"{save_path_full}/min_duration.pkl\")\n",
    "                joblib.dump(oof_action, f\"{save_path_full}/oof_pred_probs.pkl\")\n",
    "                joblib.dump(y_action, f\"{save_path_full}/y_true.pkl\")\n",
    "                joblib.dump(groups_action, f\"{save_path_full}/groups.pkl\")\n",
    "                \n",
    "                \n",
    "                del trainer\n",
    "                gc.collect()\n",
    "                \n",
    "            except Exception as e:\n",
    "                oof_action = np.zeros(len(y_action))\n",
    "                print(f\"\\tError for {action}: {e}\")\n",
    "        else:\n",
    "            oof_action = np.zeros(len(y_action))\n",
    "        \n",
    "        oof_column = np.zeros(len(label))\n",
    "        oof_column[action_mask] = oof_action\n",
    "        oof[action] = oof_column\n",
    "\n",
    "        del oof_action, action_mask, X_action, y_action, groups_action\n",
    "        gc.collect()\n",
    "\n",
    "    submission_part = predict_multiclass(oof, meta, thresholds, min_durations)\n",
    "    submission_list.append(submission_part)\n",
    "    \n",
    "    return submission_list, f1_list, thresholds, min_durations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0774ae4",
   "metadata": {
    "papermill": {
     "duration": 0.010851,
     "end_time": "2025-12-12T01:13:30.895178",
     "exception": false,
     "start_time": "2025-12-12T01:13:30.884327",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "544ca292",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T01:13:30.918285Z",
     "iopub.status.busy": "2025-12-12T01:13:30.917954Z",
     "iopub.status.idle": "2025-12-12T01:13:30.927506Z",
     "shell.execute_reply": "2025-12-12T01:13:30.926634Z"
    },
    "papermill": {
     "duration": 0.023094,
     "end_time": "2025-12-12T01:13:30.928958",
     "exception": false,
     "start_time": "2025-12-12T01:13:30.905864",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def submit(body_parts_tracked_str, switch_tr, section, thresholds, min_durations):    \n",
    "    body_parts_tracked = json.loads(body_parts_tracked_str)\n",
    "    if len(body_parts_tracked) > 5:\n",
    "        body_parts_tracked = [b for b in body_parts_tracked if b not in drop_body_parts]\n",
    "        \n",
    "    test_subset = test[test.body_parts_tracked == body_parts_tracked_str]\n",
    "    generator = generate_mouse_data(\n",
    "        test_subset, \n",
    "        'test',\n",
    "        generate_single=(switch_tr == 'single'), \n",
    "        generate_pair=(switch_tr == 'pair')\n",
    "    )\n",
    "\n",
    "    # Create FPS lookup dictionary for test set\n",
    "    fps_lookup = (\n",
    "        test_subset[['video_id', 'frames_per_second']]\n",
    "        .drop_duplicates('video_id')\n",
    "        .set_index('video_id')['frames_per_second']\n",
    "        .to_dict()\n",
    "    )\n",
    "\n",
    "    submission_list = []\n",
    "    for switch_te, data_te, meta_te, actions_te in generator:\n",
    "        assert switch_te == switch_tr\n",
    "        try:\n",
    "            fps_i = _fps_from_meta(meta_te, fps_lookup, default_fps=30.0)\n",
    "            \n",
    "            if switch_te == 'single':\n",
    "                X_te = transform_single(data_te, body_parts_tracked, fps_i)\n",
    "            else:\n",
    "                X_te = transform_pair(data_te, body_parts_tracked, fps_i)\n",
    "            del data_te\n",
    "            gc.collect()\n",
    "    \n",
    "            pred = pd.DataFrame(index=meta_te.video_frame)\n",
    "            for action in actions_te:\n",
    "                files = glob.glob(f\"{CFG.model_path}/{CFG.model_name}/{section}/{action}/*_trainer_*.pkl\")\n",
    "                if len(files) > 0:\n",
    "                    # Average predictions từ 3 models\n",
    "                    action_preds = []\n",
    "                    for model_file in files:\n",
    "                        trainer = joblib.load(model_file)\n",
    "                        action_preds.append(trainer.predict(X_te))\n",
    "                        del trainer\n",
    "                        gc.collect()\n",
    "                    \n",
    "                    pred[action] = np.mean(action_preds, axis=0)\n",
    "                \n",
    "            del X_te\n",
    "            gc.collect()\n",
    "\n",
    "            if pred.shape[1] != 0:\n",
    "                submission_part = predict_multiclass(pred, meta_te, thresholds, min_durations)\n",
    "                submission_list.append(submission_part)\n",
    "                \n",
    "        except KeyError:\n",
    "            del data_te\n",
    "            gc.collect()\n",
    "            \n",
    "    return submission_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c82dd0b",
   "metadata": {
    "papermill": {
     "duration": 0.010934,
     "end_time": "2025-12-12T01:13:30.950888",
     "exception": false,
     "start_time": "2025-12-12T01:13:30.939954",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load thresholds (validate/submit mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5a56277c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T01:13:30.973809Z",
     "iopub.status.busy": "2025-12-12T01:13:30.973508Z",
     "iopub.status.idle": "2025-12-12T01:13:30.986609Z",
     "shell.execute_reply": "2025-12-12T01:13:30.985690Z"
    },
    "papermill": {
     "duration": 0.026684,
     "end_time": "2025-12-12T01:13:30.988283",
     "exception": false,
     "start_time": "2025-12-12T01:13:30.961599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if CFG.mode == \"validate\":\n",
    "    all_thresholds = {\n",
    "        \"single\": {},\n",
    "        \"pair\": {}\n",
    "    }\n",
    "    all_min_durations = { \n",
    "        \"single\": {},\n",
    "        \"pair\": {}\n",
    "    }\n",
    "else:\n",
    "    all_thresholds = joblib.load(f\"{CFG.model_path}/{CFG.model_name}/thresholds.pkl\")\n",
    "    all_min_durations = joblib.load(f\"{CFG.model_path}/{CFG.model_name}/min_durations.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e25220f",
   "metadata": {
    "papermill": {
     "duration": 0.010724,
     "end_time": "2025-12-12T01:13:31.010051",
     "exception": false,
     "start_time": "2025-12-12T01:13:30.999327",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "864a8516",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T01:13:31.033853Z",
     "iopub.status.busy": "2025-12-12T01:13:31.033481Z",
     "iopub.status.idle": "2025-12-12T01:37:19.964402Z",
     "shell.execute_reply": "2025-12-12T01:37:19.963331Z"
    },
    "papermill": {
     "duration": 1428.945116,
     "end_time": "2025-12-12T01:37:19.966114",
     "exception": false,
     "start_time": "2025-12-12T01:13:31.020998",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1/9 Processing videos with: ['body_center', 'ear_left', 'ear_right', 'headpiece_bottombackleft', 'headpiece_bottombackright', 'headpiece_bottomfrontleft', 'headpiece_bottomfrontright', 'headpiece_topbackleft', 'headpiece_topbackright', 'headpiece_topfrontleft', 'headpiece_topfrontright', 'lateral_left', 'lateral_right', 'neck', 'nose', 'tail_base', 'tail_midpoint', 'tail_tip']\n",
      "  Processing single mouse...\n",
      "  Shape: (544859, 141)\n",
      "  Processing mouse pairs...\n",
      "  Shape: (1744248, 153)\n",
      "\n",
      "2/9 Processing videos with: ['body_center', 'ear_left', 'ear_right', 'hip_left', 'hip_right', 'lateral_left', 'lateral_right', 'nose', 'spine_1', 'spine_2', 'tail_base', 'tail_middle_1', 'tail_middle_2', 'tail_tip']\n",
      "  Processing single mouse...\n",
      "  Shape: (478728, 150)\n",
      "  Processing mouse pairs...\n",
      "  Shape: (628714, 172)\n",
      "\n",
      "3/9 Processing videos with: ['body_center', 'ear_left', 'ear_right', 'lateral_left', 'lateral_right', 'neck', 'nose', 'tail_base', 'tail_midpoint', 'tail_tip']\n",
      "  Processing single mouse...\n",
      "  Shape: (1941885, 141)\n",
      "  Processing mouse pairs...\n",
      "  Shape: (5880720, 153)\n",
      "\n",
      "4/9 Processing videos with: ['body_center', 'ear_left', 'ear_right', 'lateral_left', 'lateral_right', 'nose', 'tail_base', 'tail_tip']\n",
      "  Processing mouse pairs...\n",
      "  Shape: (2534176, 136)\n",
      "\n",
      "5/9 Processing videos with: ['body_center', 'ear_left', 'ear_right', 'lateral_left', 'lateral_right', 'nose', 'tail_base']\n",
      "  Processing mouse pairs...\n",
      "  Shape: (1849144, 121)\n",
      "\n",
      "6/9 Processing videos with: ['body_center', 'ear_left', 'ear_right', 'nose', 'tail_base']\n",
      "  Processing single mouse...\n",
      "  Shape: (708496, 115)\n",
      "  Processing mouse pairs...\n",
      "  Shape: (10212910, 97)\n",
      "\n",
      "7/9 Processing videos with: ['ear_left', 'ear_right', 'head', 'tail_base']\n",
      "  Processing single mouse...\n",
      "  Shape: (899134, 17)\n",
      "  Processing mouse pairs...\n",
      "  Shape: (899134, 19)\n",
      "\n",
      "8/9 Processing videos with: ['ear_left', 'ear_right', 'hip_left', 'hip_right', 'neck', 'nose', 'tail_base']\n",
      "  Processing single mouse...\n",
      "  Shape: (3020371, 39)\n",
      "  Processing mouse pairs...\n",
      "  Shape: (23086736, 63)\n",
      "\n",
      "9/9 Processing videos with: ['ear_left', 'ear_right', 'nose', 'tail_base', 'tail_tip']\n",
      "  Processing single mouse...\n",
      "  Shape: (329777, 28)\n",
      "  Processing mouse pairs...\n",
      "  Shape: (1774618, 39)\n"
     ]
    }
   ],
   "source": [
    "f1_list = []\n",
    "submission_list = []\n",
    "\n",
    "for section in range(1, len(body_parts_tracked_list)):\n",
    "    body_parts_tracked_str = body_parts_tracked_list[section]\n",
    "    try:\n",
    "        body_parts_tracked = json.loads(body_parts_tracked_str)\n",
    "        print(f\"\\n{section}/{len(body_parts_tracked_list)-1} Processing videos with: {body_parts_tracked}\")\n",
    "        \n",
    "        if len(body_parts_tracked) > 5:\n",
    "            body_parts_tracked = [b for b in body_parts_tracked if b not in drop_body_parts]\n",
    "    \n",
    "        train_subset = train[train.body_parts_tracked == body_parts_tracked_str]\n",
    "        \n",
    "        # Create FPS lookup dictionary\n",
    "        _fps_lookup = (\n",
    "            train_subset[['video_id', 'frames_per_second']]\n",
    "            .drop_duplicates('video_id')\n",
    "            .set_index('video_id')['frames_per_second']\n",
    "            .to_dict()\n",
    "        )\n",
    "        \n",
    "        single_mouse_list = []\n",
    "        single_mouse_label_list = []\n",
    "        single_mouse_meta_list = []\n",
    "        \n",
    "        mouse_pair_list = []\n",
    "        mouse_pair_label_list = []\n",
    "        mouse_pair_meta_list = []\n",
    "    \n",
    "        for switch, data, meta, label in generate_mouse_data(train_subset, 'train'):\n",
    "            if switch == 'single':\n",
    "                single_mouse_list.append(data)\n",
    "                single_mouse_meta_list.append(meta)\n",
    "                single_mouse_label_list.append(label)\n",
    "            else:\n",
    "                mouse_pair_list.append(data)\n",
    "                mouse_pair_meta_list.append(meta)\n",
    "                mouse_pair_label_list.append(label)\n",
    "            \n",
    "            del data, meta, label\n",
    "        gc.collect()\n",
    "    \n",
    "        if len(single_mouse_list) > 0:\n",
    "            print(\"  Processing single mouse...\")\n",
    "            single_feats_parts = []\n",
    "            for data_i, meta_i in zip(single_mouse_list, single_mouse_meta_list):\n",
    "                fps_i = _fps_from_meta(meta_i, _fps_lookup, default_fps=30.0)\n",
    "                X_i = transform_single(data_i, body_parts_tracked, fps_i).astype(np.float32)\n",
    "                single_feats_parts.append(X_i)\n",
    "                del X_i, fps_i\n",
    "            gc.collect()\n",
    "\n",
    "            X_tr = pd.concat(single_feats_parts, axis=0, ignore_index=True)\n",
    "            single_mouse_label = pd.concat(single_mouse_label_list, axis=0, ignore_index=True)\n",
    "            single_mouse_meta = pd.concat(single_mouse_meta_list, axis=0, ignore_index=True)\n",
    "            \n",
    "            print(f\"  Shape: {X_tr.shape}\")\n",
    "            \n",
    "            del single_feats_parts, single_mouse_list, single_mouse_label_list, single_mouse_meta_list\n",
    "            gc.collect()\n",
    "\n",
    "            if CFG.mode == 'validate':\n",
    "                temp_submission_list, temp_f1_list, temp_thresholds, temp_min_durations = cross_validate_classifier(\n",
    "                    X_tr, single_mouse_label, single_mouse_meta, body_parts_tracked_str, section\n",
    "                )\n",
    "                \n",
    "                if f\"{section}\" not in all_thresholds[\"single\"].keys():\n",
    "                    all_thresholds[\"single\"][f\"{section}\"] = {}\n",
    "                for k, v in temp_thresholds.items():\n",
    "                    all_thresholds[\"single\"][f\"{section}\"][k] = v   \n",
    "\n",
    "                if f\"{section}\" not in all_min_durations[\"single\"].keys():\n",
    "                    all_min_durations[\"single\"][f\"{section}\"] = {}\n",
    "                for k, v in temp_min_durations.items():\n",
    "                    all_min_durations[\"single\"][f\"{section}\"][k] = v               \n",
    "                \n",
    "                f1_list.extend(temp_f1_list)\n",
    "                submission_list.extend(temp_submission_list)\n",
    "                \n",
    "                del temp_submission_list, temp_f1_list, temp_thresholds, X_tr\n",
    "                gc.collect()\n",
    "            else:\n",
    "                temp_submission_list = submit(body_parts_tracked_str, 'single', section, \n",
    "                                              all_thresholds[\"single\"][f\"{section}\"], \n",
    "                                              all_min_durations[\"single\"][f\"{section}\"])\n",
    "                submission_list.extend(temp_submission_list)\n",
    "                \n",
    "                del temp_submission_list, X_tr\n",
    "                gc.collect()\n",
    "                \n",
    "        if len(mouse_pair_list) > 0:\n",
    "            print(\"  Processing mouse pairs...\")\n",
    "            pair_feats_parts = []\n",
    "            for data_i, meta_i in zip(mouse_pair_list, mouse_pair_meta_list):\n",
    "                fps_i = _fps_from_meta(meta_i, _fps_lookup, default_fps=30.0)\n",
    "                X_i = transform_pair(data_i, body_parts_tracked, fps_i).astype(np.float32)\n",
    "                pair_feats_parts.append(X_i)\n",
    "                del X_i, fps_i\n",
    "            gc.collect()\n",
    "\n",
    "            X_tr = pd.concat(pair_feats_parts, axis=0, ignore_index=True)\n",
    "            mouse_pair_label = pd.concat(mouse_pair_label_list, axis=0, ignore_index=True)\n",
    "            mouse_pair_meta = pd.concat(mouse_pair_meta_list, axis=0, ignore_index=True)\n",
    "            \n",
    "            print(f\"  Shape: {X_tr.shape}\")\n",
    "            \n",
    "            del pair_feats_parts, mouse_pair_list, mouse_pair_label_list, mouse_pair_meta_list\n",
    "            gc.collect()\n",
    "\n",
    "            if CFG.mode == 'validate':\n",
    "                temp_submission_list, temp_f1_list, temp_thresholds, temp_min_durations  = cross_validate_classifier(\n",
    "                    X_tr, mouse_pair_label, mouse_pair_meta, body_parts_tracked_str, section\n",
    "                )\n",
    "\n",
    "                if f\"{section}\" not in all_thresholds[\"pair\"].keys():\n",
    "                    all_thresholds[\"pair\"][f\"{section}\"] = {}\n",
    "                for k, v in temp_thresholds.items():\n",
    "                    all_thresholds[\"pair\"][f\"{section}\"][k] = v  \n",
    "\n",
    "                if f\"{section}\" not in all_min_durations[\"pair\"].keys():\n",
    "                    all_min_durations[\"pair\"][f\"{section}\"] = {}\n",
    "                for k, v in temp_min_durations.items():\n",
    "                    all_min_durations[\"pair\"][f\"{section}\"][k] = v    \n",
    "                    \n",
    "                f1_list.extend(temp_f1_list)\n",
    "                submission_list.extend(temp_submission_list)\n",
    "                \n",
    "                del temp_submission_list, temp_f1_list, temp_thresholds, X_tr\n",
    "                gc.collect()\n",
    "            else:\n",
    "                temp_submission_list = submit(body_parts_tracked_str, 'pair', section, \n",
    "                                              all_thresholds[\"pair\"][f\"{section}\"], \n",
    "                                              all_min_durations[\"pair\"][f\"{section}\"])\n",
    "                \n",
    "                submission_list.extend(temp_submission_list)\n",
    "                del temp_submission_list, X_tr\n",
    "                gc.collect()\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"\\t{e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d127c74",
   "metadata": {
    "papermill": {
     "duration": 0.01214,
     "end_time": "2025-12-12T01:37:19.990720",
     "exception": false,
     "start_time": "2025-12-12T01:37:19.978580",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Validation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2ca6429a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T01:37:20.017305Z",
     "iopub.status.busy": "2025-12-12T01:37:20.016680Z",
     "iopub.status.idle": "2025-12-12T01:37:20.023756Z",
     "shell.execute_reply": "2025-12-12T01:37:20.022723Z"
    },
    "papermill": {
     "duration": 0.023621,
     "end_time": "2025-12-12T01:37:20.026524",
     "exception": false,
     "start_time": "2025-12-12T01:37:20.002903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if CFG.mode == 'validate':\n",
    "    \n",
    "    submission = pd.concat(submission_list)\n",
    "    submission_robust = robustify(submission, train_without_mabe22, 'train')\n",
    "    print(f\"\\nCompetition metric: {score(solution, submission_robust, ''):.4f}\")\n",
    "\n",
    "    f1_df = pd.DataFrame(f1_list, columns=['body_parts_tracked_str', 'action', 'binary_f1', 'threshold'])\n",
    "    print(f\"Mean F1: {f1_df['binary_f1'].mean():.4f}\")\n",
    "    \n",
    "    save_dir = f\"{CFG.model_path}/{CFG.model_name}\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    joblib.dump(all_thresholds, f\"{save_dir}/thresholds.pkl\")\n",
    "    joblib.dump(f1_df, f\"{save_dir}/scores.pkl\")\n",
    "    joblib.dump(all_min_durations, f\"{save_dir}/min_durations.pkl\")\n",
    "    joblib.dump(submission_robust, f\"{save_dir}/oof_submission.pkl\") \n",
    "    print(f\"\\nSaved thresholds and scores to {CFG.model_path}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9e90dc5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T01:37:20.055704Z",
     "iopub.status.busy": "2025-12-12T01:37:20.055424Z",
     "iopub.status.idle": "2025-12-12T01:37:20.545233Z",
     "shell.execute_reply": "2025-12-12T01:37:20.543899Z"
    },
    "papermill": {
     "duration": 0.506465,
     "end_time": "2025-12-12T01:37:20.547372",
     "exception": false,
     "start_time": "2025-12-12T01:37:20.040907",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def detailed_error_analysis(solution_df, submission_df, title_prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Thực hiện phân tích lỗi chi tiết: Confusion Matrix, Boundary Precision, Lab Breakdown.\n",
    "    Đã sửa lỗi KeyError: 'label_key' bằng cách tạo key column trước khi tính toán.\n",
    "    \"\"\"\n",
    "    print(f\"--- {title_prefix} DETAILED ERROR ANALYSIS ---\")\n",
    "    \n",
    "    # --- BƯỚC 1: CHUẨN BỊ DỮ LIỆU CHO METRIC (POLARS + KEYS) ---\n",
    "    # Chuyển đổi sang Polars\n",
    "    sol_pl = pl.DataFrame(solution_df)\n",
    "    sub_pl = pl.DataFrame(submission_df)\n",
    "    \n",
    "    # Tạo 'label_key' cho solution: video_id_agent_id_target_id_action\n",
    "    sol_pl = sol_pl.with_columns(\n",
    "        pl.concat_str(\n",
    "            [\n",
    "                pl.col('video_id').cast(pl.Utf8),\n",
    "                pl.col('agent_id').cast(pl.Utf8),\n",
    "                pl.col('target_id').cast(pl.Utf8),\n",
    "                pl.col('action'),\n",
    "            ],\n",
    "            separator='_',\n",
    "        ).alias('label_key')\n",
    "    )\n",
    "    \n",
    "    # Tạo 'prediction_key' cho submission: video_id_agent_id_target_id_action\n",
    "    sub_pl = sub_pl.with_columns(\n",
    "        pl.concat_str(\n",
    "            [\n",
    "                pl.col('video_id').cast(pl.Utf8),\n",
    "                pl.col('agent_id').cast(pl.Utf8),\n",
    "                pl.col('target_id').cast(pl.Utf8),\n",
    "                pl.col('action'),\n",
    "            ],\n",
    "            separator='_',\n",
    "        ).alias('prediction_key')\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 1. CROSS-LAB FAILURES (Phân tích lỗi theo từng Lab)\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"\\n[1] Cross-lab Performance Breakdown:\")\n",
    "    \n",
    "    labs = sol_pl['lab_id'].unique().to_list()\n",
    "    lab_metrics = []\n",
    "    \n",
    "    for lab in labs:\n",
    "        # Lọc dữ liệu của Lab hiện tại (đã có label_key)\n",
    "        lab_sol = sol_pl.filter(pl.col('lab_id') == lab)\n",
    "        \n",
    "        # Lấy danh sách video của lab này để lọc submission tương ứng\n",
    "        lab_vid = lab_sol['video_id'].unique()\n",
    "        lab_sub = sub_pl.filter(pl.col('video_id').is_in(lab_vid))\n",
    "        \n",
    "        if len(lab_sub) == 0:\n",
    "            score = 0.0\n",
    "        else:\n",
    "            # Gọi hàm single_lab_f1 (lúc này input đã đúng chuẩn có key)\n",
    "            try:\n",
    "                score = single_lab_f1(lab_sol, lab_sub)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error calculating score for lab {lab}: {e}\")\n",
    "                score = 0.0\n",
    "        \n",
    "        lab_metrics.append({'lab_id': lab, 'f1_score': score, 'n_videos': len(lab_vid)})\n",
    "    \n",
    "    lab_df = pd.DataFrame(lab_metrics).sort_values('f1_score')\n",
    "    print(lab_df.to_string(index=False))\n",
    "    \n",
    "    if not lab_df.empty:\n",
    "        worst_lab = lab_df.iloc[0]\n",
    "        print(f\"\\n>>> Weakest Domain: {worst_lab['lab_id']} (F1: {worst_lab['f1_score']:.4f})\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 2. CONFUSION MATRIX (Dùng Pandas cho dễ xử lý logic merge)\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"\\n[2] Behavior-level Confusion Matrix & Error Types:\")\n",
    "    \n",
    "    # Lấy danh sách tất cả các behaviors\n",
    "    all_actions = sorted(list(set(solution_df.action.unique()) | set(submission_df.action.unique())))\n",
    "    action_to_idx = {a: i for i, a in enumerate(all_actions)}\n",
    "    \n",
    "    # Merge để tìm overlap\n",
    "    merged = pd.merge(\n",
    "        solution_df[['video_id', 'agent_id', 'target_id', 'action', 'start_frame', 'stop_frame']],\n",
    "        submission_df[['video_id', 'agent_id', 'target_id', 'action', 'start_frame', 'stop_frame']],\n",
    "        on=['video_id', 'agent_id', 'target_id'],\n",
    "        how='inner', suffixes=('_gt', '_pred')\n",
    "    )\n",
    "    \n",
    "    # Tính toán overlap frames\n",
    "    merged['start_overlap'] = merged[['start_frame_gt', 'start_frame_pred']].max(axis=1)\n",
    "    merged['stop_overlap'] = merged[['stop_frame_gt', 'stop_frame_pred']].min(axis=1)\n",
    "    merged['overlap_len'] = merged['stop_overlap'] - merged['start_overlap']\n",
    "    \n",
    "    valid_overlaps = merged[merged['overlap_len'] > 0].copy()\n",
    "    \n",
    "    if len(valid_overlaps) > 0:\n",
    "        cm = np.zeros((len(all_actions), len(all_actions)), dtype=int)\n",
    "        \n",
    "        for _, row in valid_overlaps.iterrows():\n",
    "            i_gt = action_to_idx[row['action_gt']]\n",
    "            i_pred = action_to_idx[row['action_pred']]\n",
    "            cm[i_gt, i_pred] += row['overlap_len']\n",
    "            \n",
    "        # Chuẩn hóa CM\n",
    "        row_sums = cm.sum(axis=1)[:, np.newaxis]\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            cm_norm = cm.astype('float') / row_sums\n",
    "        cm_norm = np.nan_to_num(cm_norm) \n",
    "\n",
    "        # Vẽ Plot\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(cm_norm, annot=True, fmt='.2f', xticklabels=all_actions, yticklabels=all_actions, cmap='Blues')\n",
    "        plt.title('Normalized Confusion Matrix (Frame-based overlap)')\n",
    "        plt.ylabel('True Action')\n",
    "        plt.xlabel('Predicted Action')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No overlapping predictions found to generate Confusion Matrix.\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 3. BOUNDARY PRECISION\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"\\n[3] Boundary Precision Analysis:\")\n",
    "    \n",
    "    if len(valid_overlaps) > 0:\n",
    "        tps = valid_overlaps[valid_overlaps['action_gt'] == valid_overlaps['action_pred']].copy()\n",
    "        \n",
    "        if len(tps) > 0:\n",
    "            tps['start_diff'] = (tps['start_frame_pred'] - tps['start_frame_gt']).abs()\n",
    "            tps['stop_diff'] = (tps['stop_frame_pred'] - tps['stop_frame_gt']).abs()\n",
    "            \n",
    "            precision_start = (tps['start_diff'] <= 5).mean() * 100\n",
    "            precision_stop = (tps['stop_diff'] <= 5).mean() * 100\n",
    "            mae_start = tps['start_diff'].mean()\n",
    "            mae_stop = tps['stop_diff'].mean()\n",
    "            \n",
    "            print(f\"  - Start Boundary Precision (<= 5 frames): {precision_start:.2f}% (MAE: {mae_start:.2f} frames)\")\n",
    "            print(f\"  - Stop Boundary Precision  (<= 5 frames): {precision_stop:.2f}% (MAE: {mae_stop:.2f} frames)\")\n",
    "            \n",
    "            if precision_start < 50 or precision_stop < 50:\n",
    "                print(\"  >>> SUGGESTION: Apply post-processing (dilation/erosion).\")\n",
    "        else:\n",
    "            print(\"  No True Positives found to analyze boundaries.\")\n",
    "    else:\n",
    "        print(\"  No overlaps found.\")\n",
    "\n",
    "# --- GỌI HÀM ---\n",
    "if CFG.mode == 'validate':\n",
    "    detailed_error_analysis(solution, submission_robust, title_prefix=\"VALIDATION\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7839669b",
   "metadata": {
    "papermill": {
     "duration": 0.012299,
     "end_time": "2025-12-12T01:37:20.572344",
     "exception": false,
     "start_time": "2025-12-12T01:37:20.560045",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "75c9b419",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T01:37:20.600194Z",
     "iopub.status.busy": "2025-12-12T01:37:20.599564Z",
     "iopub.status.idle": "2025-12-12T01:37:20.645531Z",
     "shell.execute_reply": "2025-12-12T01:37:20.644553Z"
    },
    "papermill": {
     "duration": 0.061968,
     "end_time": "2025-12-12T01:37:20.647214",
     "exception": false,
     "start_time": "2025-12-12T01:37:20.585246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if CFG.mode == 'submit':\n",
    "    if len(submission_list) > 0:\n",
    "        submission = pd.concat(submission_list)\n",
    "    else:\n",
    "        submission = pd.DataFrame(\n",
    "            dict(\n",
    "                video_id=438887472,\n",
    "                agent_id='mouse1',\n",
    "                target_id='self',\n",
    "                action='rear',\n",
    "                start_frame='278',\n",
    "                stop_frame='500'\n",
    "            ), index=[44])\n",
    "        \n",
    "    submission_robust = robustify(submission, test, 'test')\n",
    "    submission_robust.index.name = 'row_id'\n",
    "    submission_robust.to_csv('submission.csv')\n",
    "    submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "70d06869",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-12T01:37:20.674362Z",
     "iopub.status.busy": "2025-12-12T01:37:20.674025Z",
     "iopub.status.idle": "2025-12-12T01:37:20.682789Z",
     "shell.execute_reply": "2025-12-12T01:37:20.681798Z"
    },
    "papermill": {
     "duration": 0.024524,
     "end_time": "2025-12-12T01:37:20.684308",
     "exception": false,
     "start_time": "2025-12-12T01:37:20.659784",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# import joblib\n",
    "# import json\n",
    "# import gc\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.model_selection import StratifiedGroupKFold\n",
    "# from sklearn.metrics import f1_score\n",
    "# import warnings\n",
    "\n",
    "# optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# def standalone_tuner(mode='single', n_trials=30, sample_size=50000, strategy='balanced'):\n",
    "    \n",
    "#     sample_bp_str = body_parts_tracked_list[1]\n",
    "#     train_subset = train[train.body_parts_tracked == sample_bp_str]\n",
    "#     bp_list = json.loads(sample_bp_str)\n",
    "    \n",
    "#     if len(bp_list) > 5:\n",
    "#         bp_list = [b for b in bp_list if b not in drop_body_parts]\n",
    "\n",
    "#     _fps_lookup = (train_subset[['video_id', 'frames_per_second']]\n",
    "#                    .drop_duplicates('video_id')\n",
    "#                    .set_index('video_id')['frames_per_second']\n",
    "#                    .to_dict())\n",
    "\n",
    "#     X_sample_list = []\n",
    "#     y_sample_list = []\n",
    "#     groups_list = []\n",
    "    \n",
    "#     print(f\"Loading sample data for {mode} mode...\")\n",
    "#     count_rows = 0\n",
    "    \n",
    "#     gen_single = (mode == 'single')\n",
    "#     gen_pair = (mode == 'pair')\n",
    "\n",
    "#     for switch, data, meta, label in generate_mouse_data(train_subset, 'train', \n",
    "#                                                          generate_single=gen_single, \n",
    "#                                                          generate_pair=gen_pair):\n",
    "#         if switch != mode: \n",
    "#             continue\n",
    "\n",
    "#         fps_i = _fps_from_meta(meta, _fps_lookup, default_fps=30.0)\n",
    "#         if mode == 'single':\n",
    "#             X_i = transform_single(data, bp_list, fps_i).astype(np.float32)\n",
    "#         else:\n",
    "#             X_i = transform_pair(data, bp_list, fps_i).astype(np.float32)\n",
    "        \n",
    "#         target_action = None\n",
    "#         for col in label.columns:\n",
    "#             if label[col].sum() > 10:\n",
    "#                 target_action = col\n",
    "#                 break\n",
    "        \n",
    "#         if target_action is None: \n",
    "#             del data, meta, label, X_i\n",
    "#             gc.collect()\n",
    "#             continue \n",
    "            \n",
    "#         y_i = label[target_action].fillna(0).astype(int)\n",
    "#         y_i = y_i.reset_index(drop=True)\n",
    "        \n",
    "#         X_sample_list.append(X_i)\n",
    "#         y_sample_list.append(y_i)\n",
    "#         groups_list.append(meta['video_id'])\n",
    "        \n",
    "#         count_rows += len(X_i)\n",
    "#         del data, meta, label, X_i\n",
    "#         gc.collect()\n",
    "        \n",
    "#         if count_rows >= sample_size:\n",
    "#             print(f\"Loaded {count_rows} rows\")\n",
    "#             break\n",
    "            \n",
    "#     if len(X_sample_list) == 0:\n",
    "#         print(\"No valid data found\")\n",
    "#         return None\n",
    "\n",
    "#     X = pd.concat(X_sample_list, ignore_index=True)\n",
    "#     y = pd.concat(y_sample_list, ignore_index=True).astype(int)\n",
    "#     groups = pd.concat(groups_list, ignore_index=True)\n",
    "    \n",
    "#     del X_sample_list, y_sample_list, groups_list\n",
    "#     gc.collect()\n",
    "\n",
    "#     print(f\"Dataset: {X.shape}, Positive rate: {y.mean():.3f}, Unique labels: {y.unique()}\")\n",
    "    \n",
    "#     if len(np.unique(y)) < 2:\n",
    "#         print(\"Data have only 1 class. Cannot run.\")\n",
    "#         return None\n",
    "\n",
    "#     def objective(trial):\n",
    "#         if strategy == 'deep':\n",
    "#             # deeper, more complex trees\n",
    "#             params = {\n",
    "#                 'n_estimators': trial.suggest_int('n_estimators', 300, 500),\n",
    "#                 'learning_rate': trial.suggest_float('learning_rate', 0.02, 0.08, log=True),\n",
    "#                 'max_depth': trial.suggest_int('max_depth', 7, 10),\n",
    "#                 'min_child_weight': trial.suggest_int('min_child_weight', 2, 5),\n",
    "#                 'subsample': trial.suggest_float('subsample', 0.7, 0.9),\n",
    "#                 'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 0.9),\n",
    "#                 'reg_alpha': trial.suggest_float('reg_alpha', 0.01, 1.0, log=True),\n",
    "#                 'reg_lambda': trial.suggest_float('reg_lambda', 0.01, 1.0, log=True),\n",
    "#             }\n",
    "#         elif strategy == 'fast':\n",
    "#             # faster, lighter models\n",
    "#             params = {\n",
    "#                 'n_estimators': trial.suggest_int('n_estimators', 150, 300),\n",
    "#                 'learning_rate': trial.suggest_float('learning_rate', 0.05, 0.15, log=True),\n",
    "#                 'max_depth': trial.suggest_int('max_depth', 4, 6),\n",
    "#                 'min_child_weight': trial.suggest_int('min_child_weight', 5, 10),\n",
    "#                 'subsample': trial.suggest_float('subsample', 0.7, 0.9),\n",
    "#                 'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.85),\n",
    "#                 'reg_alpha': trial.suggest_float('reg_alpha', 0.1, 2.0, log=True),\n",
    "#                 'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 2.0, log=True),\n",
    "#             }\n",
    "#         elif strategy == 'regularized':\n",
    "#             # more regularization to prevent overfitting\n",
    "#             params = {\n",
    "#                 'n_estimators': trial.suggest_int('n_estimators', 200, 400),\n",
    "#                 'learning_rate': trial.suggest_float('learning_rate', 0.03, 0.1, log=True),\n",
    "#                 'max_depth': trial.suggest_int('max_depth', 5, 7),\n",
    "#                 'min_child_weight': trial.suggest_int('min_child_weight', 3, 8),\n",
    "#                 'subsample': trial.suggest_float('subsample', 0.6, 0.85),\n",
    "#                 'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.85),\n",
    "#                 'reg_alpha': trial.suggest_float('reg_alpha', 0.5, 10.0, log=True),\n",
    "#                 'reg_lambda': trial.suggest_float('reg_lambda', 0.5, 10.0, log=True),\n",
    "#                 'gamma': trial.suggest_float('gamma', 0.1, 1.0),\n",
    "#             }\n",
    "#         else:  # 'balanced'\n",
    "#             params = {\n",
    "#                 'n_estimators': trial.suggest_int('n_estimators', 200, 400),\n",
    "#                 'learning_rate': trial.suggest_float('learning_rate', 0.03, 0.1, log=True),\n",
    "#                 'max_depth': trial.suggest_int('max_depth', 5, 8),\n",
    "#                 'min_child_weight': trial.suggest_int('min_child_weight', 3, 8),\n",
    "#                 'subsample': trial.suggest_float('subsample', 0.7, 0.9),\n",
    "#                 'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 0.9),\n",
    "#                 'reg_alpha': trial.suggest_float('reg_alpha', 0.01, 2.0, log=True),\n",
    "#                 'reg_lambda': trial.suggest_float('reg_lambda', 0.01, 2.0, log=True),\n",
    "#             }\n",
    "        \n",
    "#         # Fixed params\n",
    "#         params.update({\n",
    "#             'tree_method': XGB_TREE_METHOD, \n",
    "#             'device': XGB_DEVICE,\n",
    "#             'verbosity': 0,\n",
    "#             'random_state': 42,\n",
    "#         })\n",
    "        \n",
    "#         kf = StratifiedGroupKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "#         scores = []\n",
    "        \n",
    "#         for i, (tr_idx, val_idx) in enumerate(kf.split(X, y, groups=groups)):\n",
    "#             if i >= 2: break \n",
    "            \n",
    "#             X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n",
    "#             y_tr, y_val = y.iloc[tr_idx], y.iloc[val_idx]\n",
    "            \n",
    "#             if len(np.unique(y_tr)) < 2:\n",
    "#                 continue\n",
    "            \n",
    "#             model = XGBClassifier(**params)\n",
    "#             model.fit(X_tr, y_tr, verbose=False)\n",
    "            \n",
    "#             try:\n",
    "#                 y_prob = model.predict_proba(X_val)\n",
    "#                 if y_prob.shape[1] == 2:\n",
    "#                     y_pred = (y_prob[:, 1] >= 0.5).astype(int)\n",
    "#                 else:\n",
    "#                     y_pred = np.zeros(len(y_val), dtype=int)\n",
    "#             except Exception:\n",
    "#                 y_pred = model.predict(X_val).astype(int)\n",
    "            \n",
    "#             y_val_int = y_val.astype(int)\n",
    "            \n",
    "#             scores.append(f1_score(y_val_int, y_pred, zero_division=0))\n",
    "            \n",
    "#             del model\n",
    "#             gc.collect()\n",
    "        \n",
    "#         if len(scores) == 0:\n",
    "#             return 0.0\n",
    "            \n",
    "#         return np.mean(scores)\n",
    "\n",
    "#     study = optuna.create_study(direction='maximize')\n",
    "#     study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "\n",
    "#     print(f\"\\n BEST RESULT FOR {mode.upper()} - {strategy.upper()} (F1: {study.best_value:.4f})\")\n",
    "#     print(\"\\nBest Parameters:\")\n",
    "#     for key, value in study.best_params.items():\n",
    "#         print(f\"  {key}: {value}\")\n",
    "    \n",
    "#     return study.best_params\n",
    "\n",
    "# # Chạy lại\n",
    "# best_params = standalone_tuner(mode='single', n_trials=30, strategy='deep')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 13874099,
     "isSourceIdPinned": false,
     "sourceId": 59156,
     "sourceType": "competition"
    },
    {
     "datasetId": 8865564,
     "isSourceIdPinned": true,
     "sourceId": 13918218,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8921727,
     "sourceId": 14001952,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8980173,
     "sourceId": 14100624,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8988679,
     "sourceId": 14118155,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1466.177465,
   "end_time": "2025-12-12T01:37:24.291798",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-12T01:12:58.114333",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "282cc5bea4524a2e85411f2c5634ccf5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "38da4d476a8944b09afc4bdd6eb57556": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3faac2071bec44b4b4f0e74569e278a4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "761a3511610b42bc87587141f8459ff0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_38da4d476a8944b09afc4bdd6eb57556",
       "max": 863,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_3faac2071bec44b4b4f0e74569e278a4",
       "tabbable": null,
       "tooltip": null,
       "value": 863
      }
     },
     "7c9ab9f7493c45fc91bbc086746d03e3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "849eef42770e44a1975b06ab36782b7e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b6955f8f43794845a41f0617f1fcde57",
       "placeholder": "​",
       "style": "IPY_MODEL_ef188a057c2d492089eeb54b516eb9c9",
       "tabbable": null,
       "tooltip": null,
       "value": "100%"
      }
     },
     "b6955f8f43794845a41f0617f1fcde57": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bc3898225d20419e99985e6265c7859a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f89e72ace368470798dd3a10c3137336",
       "placeholder": "​",
       "style": "IPY_MODEL_282cc5bea4524a2e85411f2c5634ccf5",
       "tabbable": null,
       "tooltip": null,
       "value": " 863/863 [00:11&lt;00:00, 99.53it/s]"
      }
     },
     "d610c02644d7428991337b19f4c3b320": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_849eef42770e44a1975b06ab36782b7e",
        "IPY_MODEL_761a3511610b42bc87587141f8459ff0",
        "IPY_MODEL_bc3898225d20419e99985e6265c7859a"
       ],
       "layout": "IPY_MODEL_7c9ab9f7493c45fc91bbc086746d03e3",
       "tabbable": null,
       "tooltip": null
      }
     },
     "ef188a057c2d492089eeb54b516eb9c9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f89e72ace368470798dd3a10c3137336": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
